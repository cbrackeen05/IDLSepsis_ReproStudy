{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12CU8xonnPY2XHAl0-oYa3klnhAAyqM7t",
      "authorship_tag": "ABX9TyPb7haxXBkq3HI9fUc6x61f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbrackeen05/IDLSepsis_ReproStudy/blob/main/OriginalPaper_Rewrite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prep Data (No1)\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "master_file = \"/content/drive/MyDrive/sepsismodel/file/master.csv\"\n",
        "\n",
        "def gen_master_feature_list():\n",
        "    # master information\n",
        "    m_set = set()\n",
        "    for i_line,line in enumerate(open(master_file)):\n",
        "        if i_line != 0:\n",
        "            data = line.strip().split(',')\n",
        "            for i,d in enumerate(data[1:]):\n",
        "                m_set.add(str(i) + d)\n",
        "    return sorted(m_set)\n",
        "\n",
        "def gen_patient_master_dict(master_list):\n",
        "    patient_master_dict = dict()\n",
        "    # master information\n",
        "    master_set = [set() for _ in range(6)]\n",
        "    for i_line,line in enumerate(open(master_file)):\n",
        "        if i_line != 0:\n",
        "            data = line.strip().split(',')\n",
        "            patient = data[0]\n",
        "            feature = ['0' for _ in range(43)]\n",
        "            for i,d in enumerate(data[1:]):\n",
        "                m = str(i) + d\n",
        "                idx = master_list.index(m)\n",
        "                feature[idx] = '1'\n",
        "            patient_master_dict[patient] = ''.join(feature)\n",
        "    content = json.dumps(patient_master_dict,indent=4,ensure_ascii=False)\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json\",'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def main():\n",
        "    master_list = gen_master_feature_list()\n",
        "    gen_patient_master_dict(master_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "qCRX0wO1rX0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prep Data (No2)\n",
        "def gen_patient_time_dict():\n",
        "    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n",
        "    patient_time_dict = dict()\n",
        "    for i_line,line in enumerate(open(vital_file)):\n",
        "        if 'event_time' not in line:\n",
        "            patient, time = line.strip().split(',')[:2]\n",
        "            patient_time_dict[patient] = max(patient_time_dict.get(patient, 0), float(time))\n",
        "    content = json.dumps(patient_time_dict,indent=4,ensure_ascii=False)\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def main():\n",
        "    gen_patient_time_dict()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "CCUqhVB-fy9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prep Data (No3)\n",
        "def gen_feature_order_dict():\n",
        "    '''\n",
        "    generate the order of value for each feature\n",
        "    '''\n",
        "\n",
        "    feature_value_order_dict = dict()\n",
        "\n",
        "    # vital information\n",
        "    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n",
        "    vital_dict = { } # key-valuelist-dict\n",
        "    for i_line,line in enumerate(open(vital_file)):\n",
        "        if i_line % 10000 == 0:\n",
        "            print(i_line)\n",
        "        # if i_line > 10000:\n",
        "        #     break\n",
        "        if i_line == 0:\n",
        "            new_line = ''\n",
        "            vis = 0\n",
        "            for c in line:\n",
        "                if c == '\"':\n",
        "                    vis = (vis + 1) % 2\n",
        "                if vis == 1 and c == ',':\n",
        "                    c = ';'\n",
        "                new_line += c\n",
        "            line = new_line\n",
        "            col_list = line.strip().split(',')[1:]\n",
        "            for col in col_list:\n",
        "                vital_dict[col] = []\n",
        "        else:\n",
        "            ctt_list = line.strip().split(',')[1:]\n",
        "            assert len(ctt_list) == len(col_list)\n",
        "            for col,ctt in zip(col_list, ctt_list):\n",
        "                if len(ctt):\n",
        "                    vital_dict[col].append(float(ctt))\n",
        "        # if i_line > 10000:\n",
        "        #    break\n",
        "        # if i_line % 10000 == 0:\n",
        "        #     print i_line\n",
        "\n",
        "    # add group info\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/similar.json\",'r') as f:\n",
        "        groups = json.loads(f.read())\n",
        "        f.close()\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as g:\n",
        "        feature_index_dict = json.loads(g.read())\n",
        "        g.close()\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/index_feature_list.json\",'r') as h:\n",
        "        index_feature_list = json.loads(h.read())\n",
        "        h.close()\n",
        "    for g in groups:\n",
        "        for k in g:\n",
        "            mg = min(g)\n",
        "            if k != mg:\n",
        "                kf = index_feature_list[k]\n",
        "                mf = index_feature_list[mg]\n",
        "                print(kf,mf)\n",
        "                try:\n",
        "                  vital_dict[mf] = vital_dict[mf] + vital_dict[kf]\n",
        "                  vital_dict.pop(kf)\n",
        "                except:\n",
        "                  pass\n",
        "    print('features', len(vital_dict))\n",
        "\n",
        "    # feature_count_dict = { k: len(v) for k,v in vital_dict.items() }\n",
        "    # py_op.mywritejson(os.path.join(args.file_dir, 'feature_count_dict.json'), feature_count_dict)\n",
        "\n",
        "    ms_list = []\n",
        "    for col in col_list:\n",
        "        if col not in vital_dict:\n",
        "            continue\n",
        "        value_list = sorted(vital_dict[col])\n",
        "        value_order_dict = dict()\n",
        "        value_minorder_dict = dict()\n",
        "        value_maxorder_dict = dict()\n",
        "        for i_value, value in enumerate(value_list):\n",
        "            if value not in value_minorder_dict:\n",
        "                value_minorder_dict[value] = i_value\n",
        "            if value == value_list[-1]:\n",
        "                value_maxorder_dict[value] = len(value_list) - 1\n",
        "                break\n",
        "            if value != value_list[i_value+1]:\n",
        "                value_maxorder_dict[value] = i_value\n",
        "        for value in value_maxorder_dict:\n",
        "            value_order_dict[value] = (value_maxorder_dict[value] + value_minorder_dict[value]) / 2.0 / len(value_list)\n",
        "        feature_value_order_dict[col] = value_order_dict\n",
        "    content = json.dumps(feature_value_order_dict,indent=4,ensure_ascii=False)\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/feature_value_order_dict.json\",'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def main():\n",
        "    gen_feature_order_dict()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #os.system('clear')\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LrU0gPogcG0",
        "outputId": "b19d276c-3cf1-4f30-8800-710bb9d4bebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Base Excess Calc Base Excess\n",
            "\"Base Excess Calc; Arterial\" Base Excess\n",
            "\"CO2 Total; Arterial\" CO2\n",
            "\"CO2 Total; Serum\" CO2\n",
            "\"Calcium; Serum\" Calcium Quant\n",
            "\"Glomerular Filtration Rate; Est\" Glomerular Filtration Rate\n",
            "Glucose WBlood Quant Glucose Stick/Meter WBlood POC\n",
            "\"HCO3; Arterial\" HCO3\n",
            "HR Apical HR\n",
            "HR Monitored HR\n",
            "\"Lymphocytes Cnt Bld Auto;Abs\" Lymph Abs Cnt\n",
            "\"Lymphocytes Cnt Bld Auto;%\" Lymph %\n",
            "Lymphocytes NFr Bld Auto Lymph %\n",
            "\"Magnesium; Serum/Plasma\" Magnesium\n",
            "\"PCO2; Arterial\" PCO2\n",
            "\"PO2; Arterial\" PO2\n",
            "PaO2 PO2\n",
            "PT Time PPP PT\n",
            "Pulse Peripheral Pulse\n",
            "Resp Rt Tot Resp Rt\n",
            "SaO2 SO2\n",
            "\"SaO2 %; Arterial\" SO2\n",
            "Temp Axillary Temp\n",
            "Temp Oral Temp\n",
            "Temp Temporal Artery Temp\n",
            "Temp Tympanic Temp\n",
            "\"Albumin; Serum\" Albumin Quant\n",
            "Neutrophil Seg Neutrophil %\n",
            "\"Nucleated RBC Ratio; Blood Auto\" Diff Nucleated RBC\n",
            "features 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prep Data (No4)\n",
        "def gen_json_data():\n",
        "    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n",
        "    patient_time_record_dict = dict()\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as h:\n",
        "        feature_index_dict = json.loads(h.read())\n",
        "        h.close()\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_value_order_dict.json\",'r') as f:\n",
        "        feature_value_order_dict = json.loads(f.read())\n",
        "        f.close()\n",
        "    feature_value_order_dict = { str(feature_index_dict[k]):v for k,v in feature_value_order_dict.items()  if 'event' not in k}\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/file/index_group_dict.json\",'r') as g:\n",
        "        index_group_dict = json.loads(g.read())\n",
        "        g.close()\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'r') as i:\n",
        "        patient_time_dict = json.loads(i.read())\n",
        "        i.close()\n",
        "    mx_time = - 100\n",
        "    for i_line, line in enumerate(open(vital_file)):\n",
        "        if i_line % 10000 == 0:\n",
        "            print('line', i_line)\n",
        "        if 'event_time' not in line:\n",
        "            data = line.strip().split(',')\n",
        "            patient, time = data[:2]\n",
        "            time = int(float(time))\n",
        "            mx_time = max(mx_time, time)\n",
        "            if patient not in patient_time_record_dict:\n",
        "                patient_time_record_dict[patient] = dict()\n",
        "            if time not in patient_time_record_dict[patient]:\n",
        "                patient_time_record_dict[patient][time] = dict()\n",
        "\n",
        "            data = data[2:]\n",
        "            vs = dict()\n",
        "            for idx, val in enumerate(data):\n",
        "                if len(val) == 0:\n",
        "                    continue\n",
        "                if str(idx) in index_group_dict:\n",
        "                    idx = index_group_dict[str(idx)]\n",
        "                value_order = feature_value_order_dict[str(idx)]\n",
        "                try:\n",
        "                  vs[idx] = value_order[val]\n",
        "                except:\n",
        "                  pass\n",
        "            patient_time_record_dict[patient][time].update(vs)\n",
        "\n",
        "    new_d = dict()\n",
        "    for p, tr in patient_time_record_dict.items():\n",
        "        new_d[p] = dict()\n",
        "        for t, vs in tr.items():\n",
        "            if mx_time > 0:\n",
        "                t = int(t - patient_time_dict[p] - 4)\n",
        "            if t < - 102:\n",
        "                continue\n",
        "            nvs = []\n",
        "            for k in sorted(vs.keys()):\n",
        "                nvs.append([k, vs[k]])\n",
        "            new_d[p][t] = nvs\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json\", 'w') as f:\n",
        "        f.write(json.dumps(new_d))\n",
        "\n",
        "def main():\n",
        "    gen_json_data()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gek8uWWA24hP",
        "outputId": "a0d55a18-021b-45aa-d06c-456f742f490d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "line 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prep Data (No5)\n",
        "def gen_patient_label_dict():\n",
        "    patient_label_dict = dict()\n",
        "    label_file = \"/content/drive/MyDrive/sepsismodel/file/label.csv\"\n",
        "    for i_line,line in enumerate(open(label_file)):\n",
        "        if i_line != 0:\n",
        "            data = line.strip().split(',')\n",
        "            patient = data[0]\n",
        "            label  = data[-1]\n",
        "            patient_label_dict[patient] = int(label)\n",
        "    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json\", 'w') as f:\n",
        "        f.write(json.dumps(patient_label_dict))\n",
        "        f.close()\n",
        "\n",
        "def main():\n",
        "    gen_patient_label_dict()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "umpx0Dsa6UW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up arguments\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser(description='SepsisModel')\n",
        "parser.add_argument('-dd',\n",
        "        '--data-dir',\n",
        "        type=str,\n",
        "        default='/content/drive/MyDrive/sepsismodel/',\n",
        "        help='data directory'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--result-dir',\n",
        "        type=str,\n",
        "        default='../result/',\n",
        "        help='result directory'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--file-dir',\n",
        "        type=str,\n",
        "        default='../file/',\n",
        "        help='useful file directory'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--vital-file',\n",
        "        type=str,\n",
        "        default='../file/vital.csv',\n",
        "        help='vital information'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--master-file',\n",
        "        type=str,\n",
        "        default='../file/master.csv',\n",
        "        help='master information'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--label-file',\n",
        "        type=str,\n",
        "        default='../file/label.csv',\n",
        "        help='label'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--model',\n",
        "        '-m',\n",
        "        type=str,\n",
        "        default='lstm',\n",
        "        help='model'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--embed-size',\n",
        "        metavar='EMBED SIZE',\n",
        "        type=int,\n",
        "        default=256,\n",
        "        help='embed size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--rnn-size',\n",
        "        metavar='rnn SIZE',\n",
        "        type=int,\n",
        "        help='rnn size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--hidden-size',\n",
        "        metavar='hidden SIZE',\n",
        "        type=int,\n",
        "        help='hidden size'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--split-num',\n",
        "        metavar='split num',\n",
        "        type=int,\n",
        "        default=5,\n",
        "        help='split num'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--split-nor',\n",
        "        metavar='split normal range',\n",
        "        type=int,\n",
        "        default=3,\n",
        "        help='split num'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--num-layers',\n",
        "        metavar='num layers',\n",
        "        type=int,\n",
        "        default=2,\n",
        "        help='num layers'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--num-code',\n",
        "        metavar='num codes',\n",
        "        type=int,\n",
        "        default=1200,\n",
        "        help='num code'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-glp',\n",
        "        metavar='use global pooling operation',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use global pooling operation'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-visit',\n",
        "        metavar='use visit as input',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use visit as input'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-value',\n",
        "        metavar='use value embedding as input',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use value embedding as input'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-cat',\n",
        "        metavar='use cat for time and value embedding',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use cat or add'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--use-trend',\n",
        "        metavar='use feature variation trend',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='use trend'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--avg-time',\n",
        "        metavar='avg time for trend, hours',\n",
        "        type=int,\n",
        "        default=4,\n",
        "        help='avg time for trend'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--seed',\n",
        "        metavar='seed',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='seed'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--set',\n",
        "        metavar='split set for training',\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help='split set'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--last-time',\n",
        "        metavar='last-time for task2',\n",
        "        type=int,\n",
        "        default=-4,\n",
        "        help='last time'\n",
        "        )\n",
        "parser.add_argument(\n",
        "        '--final',\n",
        "        metavar='final test to submit',\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help='final'\n",
        "        )\n",
        "\n",
        "parser.add_argument('--phase',\n",
        "        default='train',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='pretrain/train/test phase')\n",
        "parser.add_argument(\n",
        "        '--batch-size',\n",
        "        '-b',\n",
        "        metavar='BATCH SIZE',\n",
        "        type=int,\n",
        "        default=32,\n",
        "        help='batch size'\n",
        "        )\n",
        "parser.add_argument('--save-dir',\n",
        "        default='../../data',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='save dir')\n",
        "parser.add_argument('--resume',\n",
        "        default='',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='start from checkpoints')\n",
        "parser.add_argument('--task',\n",
        "        default='task1',\n",
        "        type=str,\n",
        "        metavar='S',\n",
        "        help='start from checkpoints')\n",
        "\n",
        "#####\n",
        "parser.add_argument('-j',\n",
        "        '--workers',\n",
        "        default=8,\n",
        "        type=int,\n",
        "        metavar='N',\n",
        "        help='number of data loading workers (default: 32)')\n",
        "parser.add_argument('--lr',\n",
        "        '--learning-rate',\n",
        "        default=0.0001,\n",
        "        type=float,\n",
        "        metavar='LR',\n",
        "        help='initial learning rate')\n",
        "parser.add_argument('--epochs',\n",
        "        default=20,\n",
        "        type=int,\n",
        "        metavar='N',\n",
        "        help='number of total epochs to run')\n",
        "parser.add_argument('--save-freq',\n",
        "        default='5',\n",
        "        type=int,\n",
        "        metavar='S',\n",
        "        help='save frequency')\n",
        "parser.add_argument('--save-pred-freq',\n",
        "        default='10',\n",
        "        type=int,\n",
        "        metavar='S',\n",
        "        help='save pred clean frequency')\n",
        "parser.add_argument('--val-freq',\n",
        "        default='5',\n",
        "        type=int,\n",
        "        metavar='S',\n",
        "        help='val frequency')\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "args.hard_mining = 0\n",
        "args.gpu = 1\n",
        "args.use_trend = max(args.use_trend, args.use_value)\n",
        "args.use_value = max(args.use_trend, args.use_value)\n",
        "args.rnn_size = args.embed_size\n",
        "args.hidden_size = args.embed_size"
      ],
      "metadata": {
        "id": "0szwMPBm8rS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function.py\n",
        "\n",
        "def save_model(p_dict, name='best.ckpt', folder='/content/drive/MyDrive/sepsismodel/result/models/'):\n",
        "    args = p_dict['args']\n",
        "    name = '{:s}-snm-{:d}-snr-{:d}-value-{:d}-trend-{:d}-cat-{:d}-lt-{:d}-size-{:d}-seed-{:d}-{:s}'.format(args.task, \n",
        "            args.split_num, args.split_nor, args.use_value, args.use_trend, \n",
        "            args.use_cat, args.last_time, args.embed_size, args.seed, name)\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    model = p_dict['model']\n",
        "    state_dict = model.state_dict()\n",
        "    for key in state_dict.keys():\n",
        "        state_dict[key] = state_dict[key].cpu()\n",
        "    all_dict = {\n",
        "            'epoch': p_dict['epoch'],\n",
        "            'args': p_dict['args'],\n",
        "            'best_metric': p_dict['best_metric'],\n",
        "            'state_dict': state_dict \n",
        "            }\n",
        "    torch.save(all_dict, os.path.join(folder, name))\n",
        "\n",
        "def load_model(p_dict, model_file):\n",
        "    all_dict = torch.load(model_file)\n",
        "    p_dict['epoch'] = all_dict['epoch']\n",
        "    # p_dict['args'] = all_dict['args']\n",
        "    p_dict['best_metric'] = all_dict['best_metric']\n",
        "    # for k,v in all_dict['state_dict'].items():\n",
        "    #     p_dict['model_dict'][k].load_state_dict(all_dict['state_dict'][k])\n",
        "    p_dict['model'].load_state_dict(all_dict['state_dict'])\n",
        "\n",
        "\n",
        "def save_segmentation_results(images, segmentations, folder='../data/middle_segmentation'):\n",
        "    stride = args.stride\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "\n",
        "    # images = images.data.cpu().numpy()\n",
        "    # segmentations = segmentations.data.cpu().numpy()\n",
        "    images = (images * 128) + 127\n",
        "    segmentations[segmentations>0] = 255\n",
        "    segmentations[segmentations<0] = 0\n",
        "\n",
        "    # print(images.shape, segmentations.shape)\n",
        "    for ii, image, seg in zip(range(len(images)), images, segmentations):\n",
        "        image = data_function.numpy_to_image(image)\n",
        "        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n",
        "        for i in range(seg.shape[1]):\n",
        "            for j in range(seg.shape[2]):\n",
        "                for k in range(3):\n",
        "                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n",
        "        seg = new_seg\n",
        "        seg = data_function.numpy_to_image(seg)\n",
        "        image.save(os.path.join(folder, str(ii) + '_image.png'))\n",
        "        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n",
        "\n",
        "\n",
        "def save_middle_results(data, folder = '../data/middle_images'):\n",
        "    stride = args.stride\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    numpy_data = [x.data.numpy() for x in data[1:]]\n",
        "    data =  data[:1] + numpy_data\n",
        "    image_names, images, word_labels, seg_labels, bbox_labels, bbox_images =  data[:6]\n",
        "    images = (images * 128) + 127\n",
        "    seg_labels = seg_labels*127 + 127\n",
        "\n",
        "\n",
        "    for ii, name, image, seg, bbox_image in zip(range(len(image_names)), image_names, images, seg_labels, bbox_images):\n",
        "        name = name.split('/')[-1]\n",
        "        image = data_function.numpy_to_image(image)\n",
        "        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n",
        "        # print(seg[0].max(),seg[0].min())\n",
        "        for i in range(seg.shape[1]):\n",
        "            for j in range(seg.shape[2]):\n",
        "                for k in range(3):\n",
        "                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n",
        "        seg = new_seg\n",
        "        seg = data_function.numpy_to_image(seg)\n",
        "        # image.save(os.path.join(folder, name))\n",
        "        # seg.save(os.path.join(folder, name.replace('image.png', 'seg.png')))\n",
        "        image.save(os.path.join(folder, str(ii) + '_image.png'))\n",
        "        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n",
        "\n",
        "        for ib,bimg in enumerate(bbox_image):\n",
        "            # print(bimg.max(), bimg.min(), bimg.dtype)\n",
        "            bimg = data_function.numpy_to_image(bimg)\n",
        "            bimg.save(os.path.join(folder, str(ii)+'_'+ str(ib) + '_bbox.png'))\n",
        "\n",
        "def save_detection_results(names, images, detect_character_output, folder='../data/test_results/'):\n",
        "    stride = args.stride\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    # images = images.data.cpu().numpy()                                      # [bs, 3, w, h]\n",
        "    images = (images * 128) + 127\n",
        "    # detect_character_output = detect_character_output.data.cpu().numpy()    # [bs, w, h, n_anchors, 5+class]\n",
        "\n",
        "    for i, name, image, bboxes in zip(range(len(names)), names, images, detect_character_output):\n",
        "        name = name.split('/')[-1]\n",
        "\n",
        "        ### 保存原图\n",
        "        # data_function.numpy_to_image(image).save(os.path.join(folder, name))\n",
        "        data_function.numpy_to_image(image).save(os.path.join(folder, str(i) + '_image.png'))\n",
        "\n",
        "        detected_bbox = detect_function.nms(bboxes)\n",
        "        # print([b[-1] for b in detected_bbox])\n",
        "        # print(len(detected_bbox))\n",
        "        image = data_function.add_bbox_to_image(image, detected_bbox)\n",
        "        # image.save(os.path.join(folder, name.replace('.png', '_bbox.png')))\n",
        "        image.save(os.path.join(folder, str(i) + '_bbox.png'))\n",
        "\n",
        "\n",
        "\n",
        "def compute_detection_metric(outputs, labels, loss_outputs,metric_dict):\n",
        "    loss_outputs[0] = loss_outputs[0].data\n",
        "    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n",
        "\n",
        "def compute_segmentation_metric(outputs, labels, loss_outputs, metric_dict):\n",
        "    loss_outputs[0] = loss_outputs[0].data\n",
        "    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n",
        "\n",
        "def compute_metric(outputs, labels, time, loss_outputs,metric_dict, phase='train'):\n",
        "    # loss_output_list, f1score_list, recall_list, precision_list):\n",
        "    if phase != 'test':\n",
        "        preds = outputs.data.cpu().numpy()\n",
        "        labels = labels.data.cpu().numpy()\n",
        "    else:\n",
        "        preds = np.array(outputs)\n",
        "\n",
        "    preds = preds.reshape(-1)\n",
        "    labels = labels.reshape(-1)\n",
        "\n",
        "    if time is not None:\n",
        "        time = time.reshape(-1)\n",
        "        assert preds.shape == time.shape\n",
        "        time = time[labels>-0.5]\n",
        "    assert preds.shape == labels.shape\n",
        "\n",
        "    preds = preds[labels>-0.5]\n",
        "    label = labels[labels>-0.5]\n",
        "\n",
        "    pred = preds > 0\n",
        "\n",
        "    assert len(pred) == len(label)\n",
        "\n",
        "    tp = (pred + label == 2).sum()\n",
        "    tn = (pred + label == 0).sum()\n",
        "    fp = (pred - label == 1).sum()\n",
        "    fn = (pred - label ==-1).sum()\n",
        "    fp = (pred - label == 1).sum()\n",
        "\n",
        "    metric_dict['tp'] = metric_dict.get('tp', 0.0) + tp\n",
        "    metric_dict['tn'] = metric_dict.get('tn', 0.0) + tn\n",
        "    metric_dict['fp'] = metric_dict.get('fp', 0.0) + fp\n",
        "    metric_dict['fn'] = metric_dict.get('fn', 0.0) + fn\n",
        "    loss = []\n",
        "    for x in loss_outputs:\n",
        "        if x == 0:\n",
        "            loss.append(x)\n",
        "        else:\n",
        "            loss.append(x.data.cpu().numpy())\n",
        "    # loss = [[x.data.cpu().numpy() for x in loss_outputs]]\n",
        "    metric_dict['loss'] = metric_dict.get('loss', []) +  [loss]\n",
        "    if phase != 'train':\n",
        "        metric_dict['preds'] = metric_dict.get('preds', []) + list(preds)\n",
        "        metric_dict['labels'] = metric_dict.get('labels', []) + list(label)\n",
        "        if time is not None:\n",
        "            metric_dict['times'] = metric_dict.get('times', []) + list(time)\n",
        "\n",
        "def compute_metric_multi_classification(outputs, labels, loss_outputs, metric_dict):\n",
        "    preds = outputs.data.cpu().numpy() > 0\n",
        "    labels = labels.data.cpu().numpy()\n",
        "    for pred, label in zip(preds, labels):\n",
        "        pred = np.argmax(pred)\n",
        "        tp = (pred == label ).sum()\n",
        "        fn = (pred != label).sum()\n",
        "        accuracy = 1.0 * tp / (tp + fn)\n",
        "        metric_dict['accuracy'] = metric_dict.get('accuracy', []) + [accuracy]\n",
        "    metric_dict['loss'] = metric_dict.get('loss', []) +  [[x.data.cpu().numpy() for x in loss_outputs]]\n",
        "\n",
        "\n",
        "def print_metric(first_line, metric_dict, phase='train'):\n",
        "    print(first_line)\n",
        "    loss_array = np.array(metric_dict['loss']).mean(0)\n",
        "    tp = metric_dict['tp']\n",
        "    tn = metric_dict['tn']\n",
        "    fp = metric_dict['fp']\n",
        "    fn = metric_dict['fn']\n",
        "    accuracy = 1.0 * (tp + tn) / (tp + tn + fp + fn)\n",
        "    recall = 1.0 * tp / (tp + fn + 10e-20)\n",
        "    precision = 1.0 * tp / (tp + fp + 10e-20)\n",
        "    f1score = 2.0 * recall * precision / (recall + precision + 10e-20)\n",
        "\n",
        "\n",
        "    \n",
        "    loss_array = loss_array.reshape(-1)\n",
        "\n",
        "    print('loss: {:3.4f}\\t pos loss: {:3.4f}\\t negloss: {:3.4f}'.format(loss_array[0], loss_array[1], loss_array[2]))\n",
        "    print('accuracy: {:3.4f}\\t f1score: {:3.4f}\\t recall: {:3.4f}\\t precision: {:3.4f}'.format(accuracy, f1score, recall, precision))\n",
        "    print('\\n')\n",
        "\n",
        "    if phase != 'train':\n",
        "        fpr, tpr, thr = metrics.roc_curve(metric_dict['labels'], metric_dict['preds'])\n",
        "        return metrics.auc(fpr, tpr)\n",
        "    else:\n",
        "        return f1score\n",
        "\n",
        "# def load_all():\n",
        "#     fo = '/content/drive/MyDrive/sepsismodel/code/models'\n",
        "#     pre = ''\n",
        "#     for fi in sorted(os.listdir(fo)):\n",
        "#         if fi[:5] != pre:\n",
        "#             print\n",
        "#             pre = fi[:5]\n",
        "#         print(os.path.join(fo, fi))\n",
        "#         x = torch.load(os.path.join(fo, fi))\n",
        "#         print (x['best_metric'], fi)\n",
        "# load_all()\n",
        "\n"
      ],
      "metadata": {
        "id": "m6iopo6W_7-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import *\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n",
        "import parse, py_op\n",
        "args = parse.args\n",
        "\n",
        "args.hard_mining = 0\n",
        "args.gpu = 1\n",
        "args.use_trend = max(args.use_trend, args.use_value)\n",
        "args.use_value = max(args.use_trend, args.use_value)\n",
        "args.rnn_size = args.embed_size\n",
        "args.hidden_size = args.embed_size\n",
        "\n",
        "def time_encoding_data(d = 512, time = 200):\n",
        "    vec = np.array([np.arange(time) * i for i in range(int(d/2))], dtype=np.float32).transpose()\n",
        "    vec = vec / vec.max() / 2\n",
        "    encoding = np.concatenate((np.sin(vec), np.cos(vec)), 1)\n",
        "    encoding = torch.from_numpy(encoding)\n",
        "    return encoding\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super ( LSTM, self ).__init__ ( )\n",
        "        self.use_cat = args.use_cat\n",
        "        self.avg_time = args.avg_time\n",
        "\n",
        "        self.embedding = nn.Embedding (opt.vocab_size, opt.embed_size )\n",
        "        self.lstm = nn.LSTM ( input_size=opt.embed_size,\n",
        "                              hidden_size=opt.hidden_size,\n",
        "                              num_layers=opt.num_layers,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        self.linear_embed = nn.Sequential (\n",
        "            nn.Linear ( opt.embed_size, opt.embed_size ),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear ( opt.embed_size, opt.embed_size ),\n",
        "        )\n",
        "        self.tv_mapping = nn.Sequential (\n",
        "            nn.Linear ( opt.embed_size , int(opt.embed_size / 2)),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "            nn.Linear ( int(opt.embed_size / 2), opt.embed_size ),\n",
        "        )\n",
        "        self.alpha = nn.Linear(args.embed_size, 1)\n",
        "\n",
        "\n",
        "        no = 1\n",
        "        if self.use_cat:\n",
        "            no += 1\n",
        "        self.output_time = nn.Sequential (\n",
        "                nn.Linear(opt.embed_size * no, opt.embed_size),\n",
        "                nn.ReLU ( ),\n",
        "        )\n",
        "\n",
        "        time = 200\n",
        "        self.time_encoding = nn.Embedding.from_pretrained(time_encoding_data(opt.embed_size, time))\n",
        "        self.time_mapping = nn.Sequential (\n",
        "            nn.Linear ( opt.embed_size, opt.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "            nn.Linear ( opt.embed_size, opt.embed_size)\n",
        "            )\n",
        "\n",
        "        self.embed_linear = nn.Sequential (\n",
        "            nn.Linear ( opt.embed_size, opt.embed_size),\n",
        "            nn.ReLU ( ),\n",
        "            # nn.Dropout ( 0.25 ),\n",
        "            # nn.Linear ( opt.embed_size, opt.embed_size),\n",
        "            # nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "        )\n",
        "        self.relu = nn.ReLU ( )\n",
        "\n",
        "        self.linears = nn.Sequential (\n",
        "            nn.Linear ( opt.hidden_size * 2, opt.rnn_size ),\n",
        "            # nn.ReLU ( ),\n",
        "            # nn.Dropout ( 0.25 ),\n",
        "            # nn.Linear ( opt.rnn_size, opt.rnn_size ),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "            nn.Linear ( opt.rnn_size, 1),\n",
        "        )\n",
        "        mn = 128\n",
        "        self.master_linear = nn.Sequential (\n",
        "            nn.Linear ( 43, mn),\n",
        "            # nn.ReLU ( ),\n",
        "            # nn.Dropout ( 0.25 ),\n",
        "            # nn.Linear ( mn, mn),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "            nn.Linear ( mn, 1),\n",
        "        )\n",
        "        self.output = nn.Sequential (\n",
        "            nn.Linear ( mn + opt.rnn_size , opt.rnn_size),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Linear ( opt.rnn_size, mn),\n",
        "            nn.ReLU ( ),\n",
        "            nn.Dropout ( 0.25 ),\n",
        "            nn.Linear ( mn, 1),\n",
        "        )\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.opt = opt\n",
        "\n",
        "    def visit_pooling(self, x, mask, time, value=None, trend=None):\n",
        "\n",
        "        output = x\n",
        "        size = output.size()\n",
        "        output = output.view(size[0] * size[1], size[2], output.size(3)) # (bs*98, 72, 512)\n",
        "        if args.use_glp:\n",
        "            output = torch.transpose(output, 1,2).contiguous() # (bs*98, 512, 72)\n",
        "            output = self.pooling(output)\n",
        "        else:\n",
        "            weight = self.alpha(output) # (bs*98, 72, 1)\n",
        "            # print weight.size()\n",
        "            weight = weight.view(size[0]*size[1], size[2])\n",
        "            # print weight.size()\n",
        "            weight = F.softmax(weight)\n",
        "            x = weight.data.cpu().numpy()\n",
        "            # print x.shape\n",
        "            weight = weight.view(size[0]*size[1], size[2], 1).expand(output.size())\n",
        "            output = weight * output # (bs*98, 512, 72)\n",
        "            # print output.size()\n",
        "            output = output.sum(1)\n",
        "            # print output.size()\n",
        "            # output = torch.transpose(output, 1,2).contiguous() \n",
        "        output = output.view(size[0], size[1], size[3])\n",
        "\n",
        "        # time encoding\n",
        "        time = - time.long()\n",
        "        time = self.time_encoding(time)\n",
        "        time = self.time_mapping(time)\n",
        "\n",
        "        if self.use_cat:\n",
        "            output = torch.cat((output, time), 2) \n",
        "            output = self.relu(output)\n",
        "            output = self.output_time(output) \n",
        "        else:\n",
        "            output = output + time\n",
        "            output = self.relu(output)\n",
        "\n",
        "\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def forward_2(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n",
        "        '''\n",
        "        task2\n",
        "        '''\n",
        "        size = list(x.size())\n",
        "        x = x.view(-1)\n",
        "        x = self.embedding( x )\n",
        "        x = self.embed_linear(x)\n",
        "        size.append(-1)\n",
        "        x = x.view(size)\n",
        "        if mask is not None:\n",
        "            x = self.visit_pooling(x, mask, time, value, trend)\n",
        "        lstm_out, _ = self.lstm( x )\n",
        "        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 512, 98)\n",
        "        mask = self.pooling(mask)\n",
        "        # print 'mask', mask.size()\n",
        "        pool_out = []\n",
        "        mask_out = []\n",
        "        time_out = []\n",
        "        time = time.data.cpu().numpy()\n",
        "        if phase == 'train':\n",
        "            start, delta = 4, 6\n",
        "        else:\n",
        "            start, delta = 1, 1\n",
        "        for i in range(start, lstm_out.size(2), delta):\n",
        "            pool_out.append(self.pooling(lstm_out[:,:, :i]))\n",
        "            mask_out.append(mask[:, i])\n",
        "            time_out.append(time[:, i])\n",
        "        pool_out.append(self.pooling(lstm_out))\n",
        "        mask_out.append(mask[:, 0])\n",
        "        time_out.append(np.zeros(size[0]) - 4)\n",
        "\n",
        "        lstm_out = torch.cat(pool_out, 2)  # (bs, 512, 98)\n",
        "        mask_out = torch.cat(mask_out, 1)  # (bs, 98)\n",
        "        time_out = np.array(time_out).transpose() # (bs, 98)\n",
        "\n",
        "        # print 'lstm_out', lstm_out.size()\n",
        "        # print 'mask_out', mask_out.size()\n",
        "        # print err\n",
        "\n",
        "        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 98, 512)\n",
        "\n",
        "        out_vital = self.linears(lstm_out)\n",
        "        size = list(out_vital.size())\n",
        "        out_vital = out_vital.view(size[:2])\n",
        "        out_master = self.master_linear(master).expand(size[:2])\n",
        "        out = out_vital + out_master\n",
        "        return out, mask_out, time_out\n",
        "\n",
        "    def forward_1(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n",
        "        # out = self.master_linear(master)\n",
        "        size = list(x.size())\n",
        "        x = x.view(-1)\n",
        "        x = self.embedding( x )\n",
        "        # print x.size()\n",
        "        x = self.embed_linear(x)\n",
        "        size.append(-1)\n",
        "        x = x.view(size)\n",
        "        if mask is not None:\n",
        "            x = self.visit_pooling(x, mask, time, value, trend)\n",
        "        lstm_out, _ = self.lstm( x )\n",
        "\n",
        "        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()\n",
        "        lstm_out = self.pooling(lstm_out)\n",
        "        lstm_out = lstm_out.view(lstm_out.size(0), -1)\n",
        "\n",
        "        out = self.linears(lstm_out) + self.master_linear(master)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n",
        "        if args.task == 'task2':\n",
        "            return self.forward_2(x, master, mask, time, phase, value, trend)\n",
        "            # return self.forward_1(x, master, mask, time, phase, value, trend)\n",
        "        else:\n",
        "            return self.forward_1(x, master, mask, time, phase, value, trend)"
      ],
      "metadata": {
        "id": "QV90XDaYDV-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Main run (No6)\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n",
        "#import parse, \n",
        "import py_op\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/sepsismodel/code')\n",
        "import loss\n",
        "#import function\n",
        "\n",
        "# import framework\n",
        "os.chdir('/content/drive/MyDrive/sepsismodel/code/loaddata')\n",
        "import dataloader\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/sepsismodel/code/models')\n",
        "import lstm\n",
        "\n",
        "def train_eval(p_dict, phase='train'):\n",
        "    epoch = p_dict['epoch']\n",
        "    model = p_dict['model']           # 模型\n",
        "    loss = p_dict['loss']             # loss 函数\n",
        "    if phase == 'train':\n",
        "        data_loader = p_dict['train_loader']        # 训练数据\n",
        "        optimizer = p_dict['optimizer']             # 优化器\n",
        "    else:\n",
        "        data_loader = p_dict['val_loader']\n",
        "\n",
        "    classification_metric_dict = dict()\n",
        "    # if args.task == 'case1':\n",
        "\n",
        "    for i,data in enumerate(tqdm(data_loader)):\n",
        "        if args.use_visit:\n",
        "            if args.gpu:\n",
        "                data = [ Variable(x.cuda()) for x in data ]\n",
        "            visits, values, mask, master, labels, times, trends  = data\n",
        "            if i == 0:\n",
        "                print('input size', visits.size())\n",
        "            output = model(visits, master, mask, times, phase, values, trends)\n",
        "        else:\n",
        "            inputs = Variable(data[0].cuda())\n",
        "            labels = Variable(data[1].cuda())\n",
        "            output = model(inputs)\n",
        "\n",
        "        # if 0:\n",
        "        if args.task == 'task2':\n",
        "            output, mask, time = output\n",
        "            labels = labels.unsqueeze(-1).expand(output.size()).contiguous()\n",
        "            labels[mask==0] = -1\n",
        "        else:\n",
        "            time = None\n",
        "\n",
        "        classification_loss_output = loss(output, labels, args.hard_mining)\n",
        "        loss_gradient = classification_loss_output[0]\n",
        "        compute_metric(output, labels, time, classification_loss_output, classification_metric_dict, phase)\n",
        "\n",
        "        # print(outputs.size(), labels.size(),data[3].size(),segment_line_output.size())\n",
        "        # print('detection', detect_character_labels.size(), detect_character_output.size())\n",
        "        # return\n",
        "\n",
        "        if phase == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss_gradient.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # if i >= 10:\n",
        "        #     break\n",
        "\n",
        "\n",
        "    print('\\nEpoch: {:d} \\t Phase: {:s} \\n'.format(epoch, phase))\n",
        "    metric = print_metric('classification', classification_metric_dict, phase)\n",
        "    if args.phase != 'train':\n",
        "        print('metric = ', metric,'\\n','\\n')\n",
        "        return\n",
        "    if phase == 'val':\n",
        "        if metric > p_dict['best_metric'][0]:\n",
        "            p_dict['best_metric'] = [metric, epoch]\n",
        "            save_model(p_dict)\n",
        "            if 0:\n",
        "            # if args.task == 'task2':\n",
        "                preds = classification_metric_dict['preds'] \n",
        "                labels = classification_metric_dict['labels'] \n",
        "                times = classification_metric_dict['times'] \n",
        "                fl = open('/content/drive/MyDrive/sepsismodel/result/tauc_label.csv', 'w')\n",
        "                fr = open('/content/drive/MyDrive/sepsismodel/result/tauc_result.csv', 'w')\n",
        "                fl.write('adm_id,last_event_time,mortality\\n')\n",
        "                fr.write('adm_id,probability\\n')\n",
        "                for i, (p,l,t) in enumerate(zip(preds, labels, times)):\n",
        "                    if i % 30:\n",
        "                        continue\n",
        "                    fl.write(str(i) + ',')\n",
        "                    fl.write(str(t) + ',')\n",
        "                    fl.write(str(int(l)) + '\\n')\n",
        "\n",
        "                    fr.write(str(i) + ',')\n",
        "                    fr.write(str(p) + '\\n')\n",
        "\n",
        "\n",
        "        print('valid: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n",
        "        print('\\t\\t\\t valid: best_metric: {:3.4f}\\t epoch: {:d}\\n'.format(p_dict['best_metric'][0], p_dict['best_metric'][1]))  \n",
        "    else:\n",
        "        print('train: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    p_dict = dict() # All the parameters\n",
        "    p_dict['args'] = args\n",
        "    args.split_nn = args.split_num + args.split_nor * 3\n",
        "    args.vocab_size = args.split_nn * 145 + 1\n",
        "    print('vocab_size', args.vocab_size)\n",
        "\n",
        "    ### load data\n",
        "    print('read data ...')\n",
        "    patient_time_record_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json')\n",
        "    patient_master_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json')\n",
        "    patient_label_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json')\n",
        "\n",
        "    patient_train = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'train.json'))))\n",
        "    patient_valid = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'val.json')))) \n",
        "\n",
        "    if len(patient_train) > len(patient_label_dict):\n",
        "        # patients = patient_time_record_dict.keys()\n",
        "        patients = list(patient_label_dict.keys())\n",
        "        n = int(0.8 * len(patients))\n",
        "        patient_train = patients[:n]\n",
        "        patient_valid = patients[n:]\n",
        "\n",
        "    print('data loading ...')\n",
        "    train_dataset  = dataloader.DataSet(\n",
        "                patient_train, \n",
        "                patient_time_record_dict,\n",
        "                patient_label_dict,\n",
        "                patient_master_dict, \n",
        "                args=args,\n",
        "                phase='train')\n",
        "    train_loader = DataLoader(\n",
        "                dataset=train_dataset, \n",
        "                batch_size=args.batch_size,\n",
        "                shuffle=True, \n",
        "                num_workers=2, \n",
        "                pin_memory=True)\n",
        "    val_dataset  = dataloader.DataSet(\n",
        "                patient_valid, \n",
        "                patient_time_record_dict,\n",
        "                patient_label_dict,\n",
        "                patient_master_dict, \n",
        "                args=args,\n",
        "                phase='val')\n",
        "    val_loader = DataLoader(\n",
        "                dataset=val_dataset, \n",
        "                batch_size=args.batch_size,\n",
        "                shuffle=False, \n",
        "                num_workers=2, \n",
        "                pin_memory=True)\n",
        "\n",
        "    p_dict['train_loader'] = train_loader\n",
        "    p_dict['val_loader'] = val_loader\n",
        "\n",
        "\n",
        "\n",
        "    cudnn.benchmark = False\n",
        "    net = LSTM(args)\n",
        "    if args.gpu:\n",
        "        net = net.cuda()\n",
        "        p_dict['loss'] = loss.Loss().cuda()\n",
        "    else:\n",
        "        p_dict['loss'] = loss.Loss()\n",
        "\n",
        "    parameters = []\n",
        "    for p in net.parameters():\n",
        "        parameters.append(p)\n",
        "    optimizer = torch.optim.Adam(parameters, lr=args.lr)\n",
        "    p_dict['optimizer'] = optimizer\n",
        "    p_dict['model'] = net\n",
        "    start_epoch = 0\n",
        "    # args.epoch = start_epoch\n",
        "    # print ('best_f1score' + str(best_f1score))\n",
        "\n",
        "    p_dict['epoch'] = 0\n",
        "    p_dict['best_metric'] = [0, 0]\n",
        "\n",
        "\n",
        "    ### resume pretrained model\n",
        "    if os.path.exists(args.resume):\n",
        "        print('resume from model ' + args.resume)\n",
        "        function.load_model(p_dict, args.resume)\n",
        "        print('best_metric', p_dict['best_metric'])\n",
        "        # return\n",
        "\n",
        "\n",
        "    if args.phase == 'train':\n",
        "\n",
        "        best_f1score = 0\n",
        "        for epoch in range(p_dict['epoch'] + 1, args.epochs):\n",
        "            p_dict['epoch'] = epoch\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = args.lr\n",
        "            train_eval(p_dict, 'train')\n",
        "            train_eval(p_dict, 'val')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utQTeEMS7gTf",
        "outputId": "65b1384c-1c77-4205-bb94-c7571de7f71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size 2031\n",
            "read data ...\n",
            "data loading ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1/3 [00:00<00:00,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6909\t pos loss: 0.3527\t negloss: 0.3382\n",
            "accuracy: 0.4583\t f1score: 0.2353\t recall: 0.2143\t precision: 0.2609\n",
            "\n",
            "\n",
            "train: metric: 0.2353\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 1 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6965\t pos loss: 0.3551\t negloss: 0.3414\n",
            "accuracy: 0.4444\t f1score: 0.1667\t recall: 0.2000\t precision: 0.1429\n",
            "\n",
            "\n",
            "valid: metric: 0.4615\t epoch: 1\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 2 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.7012\t pos loss: 0.3577\t negloss: 0.3434\n",
            "accuracy: 0.4722\t f1score: 0.2963\t recall: 0.2857\t precision: 0.3077\n",
            "\n",
            "\n",
            "train: metric: 0.2963\t epoch: 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 2 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7050\t pos loss: 0.3580\t negloss: 0.3470\n",
            "accuracy: 0.4444\t f1score: 0.2857\t recall: 0.4000\t precision: 0.2222\n",
            "\n",
            "\n",
            "valid: metric: 0.3077\t epoch: 2\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6925\t pos loss: 0.3500\t negloss: 0.3424\n",
            "accuracy: 0.5417\t f1score: 0.4000\t recall: 0.3929\t precision: 0.4074\n",
            "\n",
            "\n",
            "train: metric: 0.4000\t epoch: 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 3 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7037\t pos loss: 0.3536\t negloss: 0.3500\n",
            "accuracy: 0.3333\t f1score: 0.2500\t recall: 0.4000\t precision: 0.1818\n",
            "\n",
            "\n",
            "valid: metric: 0.3077\t epoch: 3\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 4 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6820\t pos loss: 0.3436\t negloss: 0.3384\n",
            "accuracy: 0.4861\t f1score: 0.3729\t recall: 0.3929\t precision: 0.3548\n",
            "\n",
            "\n",
            "train: metric: 0.3729\t epoch: 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 4 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7045\t pos loss: 0.3547\t negloss: 0.3498\n",
            "accuracy: 0.4444\t f1score: 0.3750\t recall: 0.6000\t precision: 0.2727\n",
            "\n",
            "\n",
            "valid: metric: 0.3692\t epoch: 4\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 5 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6878\t pos loss: 0.3416\t negloss: 0.3462\n",
            "accuracy: 0.5556\t f1score: 0.5000\t recall: 0.5714\t precision: 0.4444\n",
            "\n",
            "\n",
            "train: metric: 0.5000\t epoch: 5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 5 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7043\t pos loss: 0.3511\t negloss: 0.3532\n",
            "accuracy: 0.5000\t f1score: 0.4000\t recall: 0.6000\t precision: 0.3000\n",
            "\n",
            "\n",
            "valid: metric: 0.4000\t epoch: 5\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 6 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6932\t pos loss: 0.3454\t negloss: 0.3478\n",
            "accuracy: 0.4167\t f1score: 0.3438\t recall: 0.3929\t precision: 0.3056\n",
            "\n",
            "\n",
            "train: metric: 0.3438\t epoch: 6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 6 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7107\t pos loss: 0.3535\t negloss: 0.3572\n",
            "accuracy: 0.1667\t f1score: 0.1176\t recall: 0.2000\t precision: 0.0833\n",
            "\n",
            "\n",
            "valid: metric: 0.0923\t epoch: 6\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 7 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6880\t pos loss: 0.3396\t negloss: 0.3484\n",
            "accuracy: 0.5556\t f1score: 0.5294\t recall: 0.6429\t precision: 0.4500\n",
            "\n",
            "\n",
            "train: metric: 0.5294\t epoch: 7\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 7 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7091\t pos loss: 0.3534\t negloss: 0.3557\n",
            "accuracy: 0.2778\t f1score: 0.0000\t recall: 0.0000\t precision: 0.0000\n",
            "\n",
            "\n",
            "valid: metric: 0.2308\t epoch: 7\n",
            "\n",
            "\t\t\t valid: best_metric: 0.4615\t epoch: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 8 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6848\t pos loss: 0.3430\t negloss: 0.3418\n",
            "accuracy: 0.5000\t f1score: 0.4375\t recall: 0.5000\t precision: 0.3889\n",
            "\n",
            "\n",
            "train: metric: 0.4375\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 8 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6850\t pos loss: 0.3322\t negloss: 0.3528\n",
            "accuracy: 0.5000\t f1score: 0.4706\t recall: 0.8000\t precision: 0.3333\n",
            "\n",
            "\n",
            "valid: metric: 0.6769\t epoch: 8\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 9 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6846\t pos loss: 0.3381\t negloss: 0.3465\n",
            "accuracy: 0.5556\t f1score: 0.5000\t recall: 0.5714\t precision: 0.4444\n",
            "\n",
            "\n",
            "train: metric: 0.5000\t epoch: 9\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 9 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7085\t pos loss: 0.3534\t negloss: 0.3550\n",
            "accuracy: 0.4444\t f1score: 0.3750\t recall: 0.6000\t precision: 0.2727\n",
            "\n",
            "\n",
            "valid: metric: 0.3538\t epoch: 9\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 10 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6867\t pos loss: 0.3408\t negloss: 0.3458\n",
            "accuracy: 0.5972\t f1score: 0.5085\t recall: 0.5357\t precision: 0.4839\n",
            "\n",
            "\n",
            "train: metric: 0.5085\t epoch: 10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 10 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6846\t pos loss: 0.3388\t negloss: 0.3458\n",
            "accuracy: 0.5556\t f1score: 0.4286\t recall: 0.6000\t precision: 0.3333\n",
            "\n",
            "\n",
            "valid: metric: 0.6615\t epoch: 10\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 11 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6761\t pos loss: 0.3405\t negloss: 0.3357\n",
            "accuracy: 0.6389\t f1score: 0.5667\t recall: 0.6071\t precision: 0.5312\n",
            "\n",
            "\n",
            "train: metric: 0.5667\t epoch: 11\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 11 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7015\t pos loss: 0.3488\t negloss: 0.3527\n",
            "accuracy: 0.3333\t f1score: 0.2500\t recall: 0.4000\t precision: 0.1818\n",
            "\n",
            "\n",
            "valid: metric: 0.3846\t epoch: 11\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 12 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6875\t pos loss: 0.3454\t negloss: 0.3421\n",
            "accuracy: 0.5972\t f1score: 0.5397\t recall: 0.6071\t precision: 0.4857\n",
            "\n",
            "\n",
            "train: metric: 0.5397\t epoch: 12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 12 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6823\t pos loss: 0.3316\t negloss: 0.3507\n",
            "accuracy: 0.5000\t f1score: 0.5263\t recall: 1.0000\t precision: 0.3571\n",
            "\n",
            "\n",
            "valid: metric: 0.6769\t epoch: 12\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 13 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6599\t pos loss: 0.3245\t negloss: 0.3354\n",
            "accuracy: 0.7361\t f1score: 0.6780\t recall: 0.7143\t precision: 0.6452\n",
            "\n",
            "\n",
            "train: metric: 0.6780\t epoch: 13\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 13 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6948\t pos loss: 0.3353\t negloss: 0.3595\n",
            "accuracy: 0.3889\t f1score: 0.3529\t recall: 0.6000\t precision: 0.2500\n",
            "\n",
            "\n",
            "valid: metric: 0.4923\t epoch: 13\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  7.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n",
            "\n",
            "Epoch: 14 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6592\t pos loss: 0.3177\t negloss: 0.3415\n",
            "accuracy: 0.6389\t f1score: 0.5667\t recall: 0.6071\t precision: 0.5312\n",
            "\n",
            "\n",
            "train: metric: 0.5667\t epoch: 14\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 14 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6918\t pos loss: 0.3240\t negloss: 0.3679\n",
            "accuracy: 0.4444\t f1score: 0.4444\t recall: 0.8000\t precision: 0.3077\n",
            "\n",
            "\n",
            "valid: metric: 0.5231\t epoch: 14\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 15 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6445\t pos loss: 0.3089\t negloss: 0.3355\n",
            "accuracy: 0.7361\t f1score: 0.6885\t recall: 0.7500\t precision: 0.6364\n",
            "\n",
            "\n",
            "train: metric: 0.6885\t epoch: 15\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 15 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6850\t pos loss: 0.3323\t negloss: 0.3527\n",
            "accuracy: 0.4444\t f1score: 0.2857\t recall: 0.4000\t precision: 0.2222\n",
            "\n",
            "\n",
            "valid: metric: 0.6154\t epoch: 15\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 3/3 [00:00<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 16 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6237\t pos loss: 0.3132\t negloss: 0.3105\n",
            "accuracy: 0.7639\t f1score: 0.6792\t recall: 0.6429\t precision: 0.7200\n",
            "\n",
            "\n",
            "train: metric: 0.6792\t epoch: 16\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 16 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6839\t pos loss: 0.3541\t negloss: 0.3299\n",
            "accuracy: 0.5556\t f1score: 0.3333\t recall: 0.4000\t precision: 0.2857\n",
            "\n",
            "\n",
            "valid: metric: 0.4923\t epoch: 16\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 3/3 [00:00<00:00,  4.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 17 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.6111\t pos loss: 0.3207\t negloss: 0.2904\n",
            "accuracy: 0.7778\t f1score: 0.6800\t recall: 0.6071\t precision: 0.7727\n",
            "\n",
            "\n",
            "train: metric: 0.6800\t epoch: 17\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 17 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7022\t pos loss: 0.3450\t negloss: 0.3572\n",
            "accuracy: 0.5000\t f1score: 0.3077\t recall: 0.4000\t precision: 0.2500\n",
            "\n",
            "\n",
            "valid: metric: 0.5077\t epoch: 17\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 18 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.5626\t pos loss: 0.2707\t negloss: 0.2918\n",
            "accuracy: 0.7778\t f1score: 0.7143\t recall: 0.7143\t precision: 0.7143\n",
            "\n",
            "\n",
            "train: metric: 0.7143\t epoch: 18\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n",
            "\n",
            "Epoch: 18 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.6589\t pos loss: 0.3792\t negloss: 0.2796\n",
            "accuracy: 0.7222\t f1score: 0.4444\t recall: 0.4000\t precision: 0.5000\n",
            "\n",
            "\n",
            "valid: metric: 0.5077\t epoch: 18\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  6.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([32, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 3/3 [00:00<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 19 \t Phase: train \n",
            "\n",
            "classification\n",
            "loss: 0.5993\t pos loss: 0.3202\t negloss: 0.2791\n",
            "accuracy: 0.7778\t f1score: 0.6522\t recall: 0.5357\t precision: 0.8333\n",
            "\n",
            "\n",
            "train: metric: 0.6522\t epoch: 19\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size torch.Size([18, 98, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 19 \t Phase: val \n",
            "\n",
            "classification\n",
            "loss: 0.7668\t pos loss: 0.3735\t negloss: 0.3934\n",
            "accuracy: 0.5000\t f1score: 0.3077\t recall: 0.4000\t precision: 0.2500\n",
            "\n",
            "\n",
            "valid: metric: 0.4462\t epoch: 19\n",
            "\n",
            "\t\t\t valid: best_metric: 0.6769\t epoch: 8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hI9JQxzJ5N6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
