{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12CU8xonnPY2XHAl0-oYa3klnhAAyqM7t","authorship_tag":"ABX9TyPb7haxXBkq3HI9fUc6x61f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["#Prep Data (No1)\n","import os\n","import sys\n","import json\n","\n","master_file = \"/content/drive/MyDrive/sepsismodel/file/master.csv\"\n","\n","def gen_master_feature_list():\n","    # master information\n","    m_set = set()\n","    for i_line,line in enumerate(open(master_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            for i,d in enumerate(data[1:]):\n","                m_set.add(str(i) + d)\n","    return sorted(m_set)\n","\n","def gen_patient_master_dict(master_list):\n","    patient_master_dict = dict()\n","    # master information\n","    master_set = [set() for _ in range(6)]\n","    for i_line,line in enumerate(open(master_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            patient = data[0]\n","            feature = ['0' for _ in range(43)]\n","            for i,d in enumerate(data[1:]):\n","                m = str(i) + d\n","                idx = master_list.index(m)\n","                feature[idx] = '1'\n","            patient_master_dict[patient] = ''.join(feature)\n","    content = json.dumps(patient_master_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    master_list = gen_master_feature_list()\n","    gen_patient_master_dict(master_list)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"qCRX0wO1rX0E","executionInfo":{"status":"ok","timestamp":1681546641075,"user_tz":420,"elapsed":3177,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#Prep Data (No2)\n","def gen_patient_time_dict():\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    patient_time_dict = dict()\n","    for i_line,line in enumerate(open(vital_file)):\n","        if 'event_time' not in line:\n","            patient, time = line.strip().split(',')[:2]\n","            patient_time_dict[patient] = max(patient_time_dict.get(patient, 0), float(time))\n","    content = json.dumps(patient_time_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    gen_patient_time_dict()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"CCUqhVB-fy9O","executionInfo":{"status":"ok","timestamp":1681546645984,"user_tz":420,"elapsed":2016,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Prep Data (No3)\n","def gen_feature_order_dict():\n","    '''\n","    generate the order of value for each feature\n","    '''\n","\n","    feature_value_order_dict = dict()\n","\n","    # vital information\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    vital_dict = { } # key-valuelist-dict\n","    for i_line,line in enumerate(open(vital_file)):\n","        if i_line % 10000 == 0:\n","            print(i_line)\n","        # if i_line > 10000:\n","        #     break\n","        if i_line == 0:\n","            new_line = ''\n","            vis = 0\n","            for c in line:\n","                if c == '\"':\n","                    vis = (vis + 1) % 2\n","                if vis == 1 and c == ',':\n","                    c = ';'\n","                new_line += c\n","            line = new_line\n","            col_list = line.strip().split(',')[1:]\n","            for col in col_list:\n","                vital_dict[col] = []\n","        else:\n","            ctt_list = line.strip().split(',')[1:]\n","            assert len(ctt_list) == len(col_list)\n","            for col,ctt in zip(col_list, ctt_list):\n","                if len(ctt):\n","                    vital_dict[col].append(float(ctt))\n","        # if i_line > 10000:\n","        #    break\n","        # if i_line % 10000 == 0:\n","        #     print i_line\n","\n","    # add group info\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/similar.json\",'r') as f:\n","        groups = json.loads(f.read())\n","        f.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as g:\n","        feature_index_dict = json.loads(g.read())\n","        g.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/index_feature_list.json\",'r') as h:\n","        index_feature_list = json.loads(h.read())\n","        h.close()\n","    for g in groups:\n","        for k in g:\n","            mg = min(g)\n","            if k != mg:\n","                kf = index_feature_list[k]\n","                mf = index_feature_list[mg]\n","                print(kf,mf)\n","                try:\n","                  vital_dict[mf] = vital_dict[mf] + vital_dict[kf]\n","                  vital_dict.pop(kf)\n","                except:\n","                  pass\n","    print('features', len(vital_dict))\n","\n","    # feature_count_dict = { k: len(v) for k,v in vital_dict.items() }\n","    # py_op.mywritejson(os.path.join(args.file_dir, 'feature_count_dict.json'), feature_count_dict)\n","\n","    ms_list = []\n","    for col in col_list:\n","        if col not in vital_dict:\n","            continue\n","        value_list = sorted(vital_dict[col])\n","        value_order_dict = dict()\n","        value_minorder_dict = dict()\n","        value_maxorder_dict = dict()\n","        for i_value, value in enumerate(value_list):\n","            if value not in value_minorder_dict:\n","                value_minorder_dict[value] = i_value\n","            if value == value_list[-1]:\n","                value_maxorder_dict[value] = len(value_list) - 1\n","                break\n","            if value != value_list[i_value+1]:\n","                value_maxorder_dict[value] = i_value\n","        for value in value_maxorder_dict:\n","            value_order_dict[value] = (value_maxorder_dict[value] + value_minorder_dict[value]) / 2.0 / len(value_list)\n","        feature_value_order_dict[col] = value_order_dict\n","    content = json.dumps(feature_value_order_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/feature_value_order_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    gen_feature_order_dict()\n","\n","if __name__ == '__main__':\n","    #os.system('clear')\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LrU0gPogcG0","executionInfo":{"status":"ok","timestamp":1681546649684,"user_tz":420,"elapsed":3705,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"b19d276c-3cf1-4f30-8800-710bb9d4bebd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","Base Excess Calc Base Excess\n","\"Base Excess Calc; Arterial\" Base Excess\n","\"CO2 Total; Arterial\" CO2\n","\"CO2 Total; Serum\" CO2\n","\"Calcium; Serum\" Calcium Quant\n","\"Glomerular Filtration Rate; Est\" Glomerular Filtration Rate\n","Glucose WBlood Quant Glucose Stick/Meter WBlood POC\n","\"HCO3; Arterial\" HCO3\n","HR Apical HR\n","HR Monitored HR\n","\"Lymphocytes Cnt Bld Auto;Abs\" Lymph Abs Cnt\n","\"Lymphocytes Cnt Bld Auto;%\" Lymph %\n","Lymphocytes NFr Bld Auto Lymph %\n","\"Magnesium; Serum/Plasma\" Magnesium\n","\"PCO2; Arterial\" PCO2\n","\"PO2; Arterial\" PO2\n","PaO2 PO2\n","PT Time PPP PT\n","Pulse Peripheral Pulse\n","Resp Rt Tot Resp Rt\n","SaO2 SO2\n","\"SaO2 %; Arterial\" SO2\n","Temp Axillary Temp\n","Temp Oral Temp\n","Temp Temporal Artery Temp\n","Temp Tympanic Temp\n","\"Albumin; Serum\" Albumin Quant\n","Neutrophil Seg Neutrophil %\n","\"Nucleated RBC Ratio; Blood Auto\" Diff Nucleated RBC\n","features 116\n"]}]},{"cell_type":"code","source":["#Prep Data (No4)\n","def gen_json_data():\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    patient_time_record_dict = dict()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as h:\n","        feature_index_dict = json.loads(h.read())\n","        h.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_value_order_dict.json\",'r') as f:\n","        feature_value_order_dict = json.loads(f.read())\n","        f.close()\n","    feature_value_order_dict = { str(feature_index_dict[k]):v for k,v in feature_value_order_dict.items()  if 'event' not in k}\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/index_group_dict.json\",'r') as g:\n","        index_group_dict = json.loads(g.read())\n","        g.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'r') as i:\n","        patient_time_dict = json.loads(i.read())\n","        i.close()\n","    mx_time = - 100\n","    for i_line, line in enumerate(open(vital_file)):\n","        if i_line % 10000 == 0:\n","            print('line', i_line)\n","        if 'event_time' not in line:\n","            data = line.strip().split(',')\n","            patient, time = data[:2]\n","            time = int(float(time))\n","            mx_time = max(mx_time, time)\n","            if patient not in patient_time_record_dict:\n","                patient_time_record_dict[patient] = dict()\n","            if time not in patient_time_record_dict[patient]:\n","                patient_time_record_dict[patient][time] = dict()\n","\n","            data = data[2:]\n","            vs = dict()\n","            for idx, val in enumerate(data):\n","                if len(val) == 0:\n","                    continue\n","                if str(idx) in index_group_dict:\n","                    idx = index_group_dict[str(idx)]\n","                value_order = feature_value_order_dict[str(idx)]\n","                try:\n","                  vs[idx] = value_order[val]\n","                except:\n","                  pass\n","            patient_time_record_dict[patient][time].update(vs)\n","\n","    new_d = dict()\n","    for p, tr in patient_time_record_dict.items():\n","        new_d[p] = dict()\n","        for t, vs in tr.items():\n","            if mx_time > 0:\n","                t = int(t - patient_time_dict[p] - 4)\n","            if t < - 102:\n","                continue\n","            nvs = []\n","            for k in sorted(vs.keys()):\n","                nvs.append([k, vs[k]])\n","            new_d[p][t] = nvs\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json\", 'w') as f:\n","        f.write(json.dumps(new_d))\n","\n","def main():\n","    gen_json_data()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gek8uWWA24hP","executionInfo":{"status":"ok","timestamp":1681546652477,"user_tz":420,"elapsed":2796,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"a0d55a18-021b-45aa-d06c-456f742f490d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["line 0\n"]}]},{"cell_type":"code","source":["#Prep Data (No5)\n","def gen_patient_label_dict():\n","    patient_label_dict = dict()\n","    label_file = \"/content/drive/MyDrive/sepsismodel/file/label.csv\"\n","    for i_line,line in enumerate(open(label_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            patient = data[0]\n","            label  = data[-1]\n","            patient_label_dict[patient] = int(label)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json\", 'w') as f:\n","        f.write(json.dumps(patient_label_dict))\n","        f.close()\n","\n","def main():\n","    gen_patient_label_dict()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"umpx0Dsa6UW_","executionInfo":{"status":"ok","timestamp":1681546655383,"user_tz":420,"elapsed":2919,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Set up arguments\n","\n","import argparse\n","parser = argparse.ArgumentParser(description='SepsisModel')\n","parser.add_argument('-dd',\n","        '--data-dir',\n","        type=str,\n","        default='/content/drive/MyDrive/sepsismodel/',\n","        help='data directory'\n","        )\n","parser.add_argument(\n","        '--result-dir',\n","        type=str,\n","        default='../result/',\n","        help='result directory'\n","        )\n","parser.add_argument(\n","        '--file-dir',\n","        type=str,\n","        default='../file/',\n","        help='useful file directory'\n","        )\n","parser.add_argument(\n","        '--vital-file',\n","        type=str,\n","        default='../file/vital.csv',\n","        help='vital information'\n","        )\n","parser.add_argument(\n","        '--master-file',\n","        type=str,\n","        default='../file/master.csv',\n","        help='master information'\n","        )\n","parser.add_argument(\n","        '--label-file',\n","        type=str,\n","        default='../file/label.csv',\n","        help='label'\n","        )\n","parser.add_argument(\n","        '--model',\n","        '-m',\n","        type=str,\n","        default='lstm',\n","        help='model'\n","        )\n","parser.add_argument(\n","        '--embed-size',\n","        metavar='EMBED SIZE',\n","        type=int,\n","        default=256,\n","        help='embed size'\n","        )\n","parser.add_argument(\n","        '--rnn-size',\n","        metavar='rnn SIZE',\n","        type=int,\n","        help='rnn size'\n","        )\n","parser.add_argument(\n","        '--hidden-size',\n","        metavar='hidden SIZE',\n","        type=int,\n","        help='hidden size'\n","        )\n","parser.add_argument(\n","        '--split-num',\n","        metavar='split num',\n","        type=int,\n","        default=5,\n","        help='split num'\n","        )\n","parser.add_argument(\n","        '--split-nor',\n","        metavar='split normal range',\n","        type=int,\n","        default=3,\n","        help='split num'\n","        )\n","parser.add_argument(\n","        '--num-layers',\n","        metavar='num layers',\n","        type=int,\n","        default=2,\n","        help='num layers'\n","        )\n","parser.add_argument(\n","        '--num-code',\n","        metavar='num codes',\n","        type=int,\n","        default=1200,\n","        help='num code'\n","        )\n","parser.add_argument(\n","        '--use-glp',\n","        metavar='use global pooling operation',\n","        type=int,\n","        default=1,\n","        help='use global pooling operation'\n","        )\n","parser.add_argument(\n","        '--use-visit',\n","        metavar='use visit as input',\n","        type=int,\n","        default=1,\n","        help='use visit as input'\n","        )\n","parser.add_argument(\n","        '--use-value',\n","        metavar='use value embedding as input',\n","        type=int,\n","        default=1,\n","        help='use value embedding as input'\n","        )\n","parser.add_argument(\n","        '--use-cat',\n","        metavar='use cat for time and value embedding',\n","        type=int,\n","        default=1,\n","        help='use cat or add'\n","        )\n","parser.add_argument(\n","        '--use-trend',\n","        metavar='use feature variation trend',\n","        type=int,\n","        default=1,\n","        help='use trend'\n","        )\n","parser.add_argument(\n","        '--avg-time',\n","        metavar='avg time for trend, hours',\n","        type=int,\n","        default=4,\n","        help='avg time for trend'\n","        )\n","parser.add_argument(\n","        '--seed',\n","        metavar='seed',\n","        type=int,\n","        default=1,\n","        help='seed'\n","        )\n","parser.add_argument(\n","        '--set',\n","        metavar='split set for training',\n","        type=int,\n","        default=0,\n","        help='split set'\n","        )\n","parser.add_argument(\n","        '--last-time',\n","        metavar='last-time for task2',\n","        type=int,\n","        default=-4,\n","        help='last time'\n","        )\n","parser.add_argument(\n","        '--final',\n","        metavar='final test to submit',\n","        type=int,\n","        default=0,\n","        help='final'\n","        )\n","\n","parser.add_argument('--phase',\n","        default='train',\n","        type=str,\n","        metavar='S',\n","        help='pretrain/train/test phase')\n","parser.add_argument(\n","        '--batch-size',\n","        '-b',\n","        metavar='BATCH SIZE',\n","        type=int,\n","        default=32,\n","        help='batch size'\n","        )\n","parser.add_argument('--save-dir',\n","        default='../../data',\n","        type=str,\n","        metavar='S',\n","        help='save dir')\n","parser.add_argument('--resume',\n","        default='',\n","        type=str,\n","        metavar='S',\n","        help='start from checkpoints')\n","parser.add_argument('--task',\n","        default='task1',\n","        type=str,\n","        metavar='S',\n","        help='start from checkpoints')\n","\n","#####\n","parser.add_argument('-j',\n","        '--workers',\n","        default=8,\n","        type=int,\n","        metavar='N',\n","        help='number of data loading workers (default: 32)')\n","parser.add_argument('--lr',\n","        '--learning-rate',\n","        default=0.0001,\n","        type=float,\n","        metavar='LR',\n","        help='initial learning rate')\n","parser.add_argument('--epochs',\n","        default=20,\n","        type=int,\n","        metavar='N',\n","        help='number of total epochs to run')\n","parser.add_argument('--save-freq',\n","        default='5',\n","        type=int,\n","        metavar='S',\n","        help='save frequency')\n","parser.add_argument('--save-pred-freq',\n","        default='10',\n","        type=int,\n","        metavar='S',\n","        help='save pred clean frequency')\n","parser.add_argument('--val-freq',\n","        default='5',\n","        type=int,\n","        metavar='S',\n","        help='val frequency')\n","args, unknown = parser.parse_known_args()\n","\n","args.hard_mining = 0\n","args.gpu = 1\n","args.use_trend = max(args.use_trend, args.use_value)\n","args.use_value = max(args.use_trend, args.use_value)\n","args.rnn_size = args.embed_size\n","args.hidden_size = args.embed_size"],"metadata":{"id":"0szwMPBm8rS3","executionInfo":{"status":"ok","timestamp":1681546655384,"user_tz":420,"elapsed":5,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Function.py\n","\n","def save_model(p_dict, name='best.ckpt', folder='/content/drive/MyDrive/sepsismodel/result/models/'):\n","    args = p_dict['args']\n","    name = '{:s}-snm-{:d}-snr-{:d}-value-{:d}-trend-{:d}-cat-{:d}-lt-{:d}-size-{:d}-seed-{:d}-{:s}'.format(args.task, \n","            args.split_num, args.split_nor, args.use_value, args.use_trend, \n","            args.use_cat, args.last_time, args.embed_size, args.seed, name)\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    model = p_dict['model']\n","    state_dict = model.state_dict()\n","    for key in state_dict.keys():\n","        state_dict[key] = state_dict[key].cpu()\n","    all_dict = {\n","            'epoch': p_dict['epoch'],\n","            'args': p_dict['args'],\n","            'best_metric': p_dict['best_metric'],\n","            'state_dict': state_dict \n","            }\n","    torch.save(all_dict, os.path.join(folder, name))\n","\n","def load_model(p_dict, model_file):\n","    all_dict = torch.load(model_file)\n","    p_dict['epoch'] = all_dict['epoch']\n","    # p_dict['args'] = all_dict['args']\n","    p_dict['best_metric'] = all_dict['best_metric']\n","    # for k,v in all_dict['state_dict'].items():\n","    #     p_dict['model_dict'][k].load_state_dict(all_dict['state_dict'][k])\n","    p_dict['model'].load_state_dict(all_dict['state_dict'])\n","\n","\n","def save_segmentation_results(images, segmentations, folder='../data/middle_segmentation'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","\n","    # images = images.data.cpu().numpy()\n","    # segmentations = segmentations.data.cpu().numpy()\n","    images = (images * 128) + 127\n","    segmentations[segmentations>0] = 255\n","    segmentations[segmentations<0] = 0\n","\n","    # print(images.shape, segmentations.shape)\n","    for ii, image, seg in zip(range(len(images)), images, segmentations):\n","        image = data_function.numpy_to_image(image)\n","        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n","        for i in range(seg.shape[1]):\n","            for j in range(seg.shape[2]):\n","                for k in range(3):\n","                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n","        seg = new_seg\n","        seg = data_function.numpy_to_image(seg)\n","        image.save(os.path.join(folder, str(ii) + '_image.png'))\n","        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n","\n","\n","def save_middle_results(data, folder = '../data/middle_images'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    numpy_data = [x.data.numpy() for x in data[1:]]\n","    data =  data[:1] + numpy_data\n","    image_names, images, word_labels, seg_labels, bbox_labels, bbox_images =  data[:6]\n","    images = (images * 128) + 127\n","    seg_labels = seg_labels*127 + 127\n","\n","\n","    for ii, name, image, seg, bbox_image in zip(range(len(image_names)), image_names, images, seg_labels, bbox_images):\n","        name = name.split('/')[-1]\n","        image = data_function.numpy_to_image(image)\n","        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n","        # print(seg[0].max(),seg[0].min())\n","        for i in range(seg.shape[1]):\n","            for j in range(seg.shape[2]):\n","                for k in range(3):\n","                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n","        seg = new_seg\n","        seg = data_function.numpy_to_image(seg)\n","        # image.save(os.path.join(folder, name))\n","        # seg.save(os.path.join(folder, name.replace('image.png', 'seg.png')))\n","        image.save(os.path.join(folder, str(ii) + '_image.png'))\n","        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n","\n","        for ib,bimg in enumerate(bbox_image):\n","            # print(bimg.max(), bimg.min(), bimg.dtype)\n","            bimg = data_function.numpy_to_image(bimg)\n","            bimg.save(os.path.join(folder, str(ii)+'_'+ str(ib) + '_bbox.png'))\n","\n","def save_detection_results(names, images, detect_character_output, folder='../data/test_results/'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    # images = images.data.cpu().numpy()                                      # [bs, 3, w, h]\n","    images = (images * 128) + 127\n","    # detect_character_output = detect_character_output.data.cpu().numpy()    # [bs, w, h, n_anchors, 5+class]\n","\n","    for i, name, image, bboxes in zip(range(len(names)), names, images, detect_character_output):\n","        name = name.split('/')[-1]\n","\n","        ### 保存原图\n","        # data_function.numpy_to_image(image).save(os.path.join(folder, name))\n","        data_function.numpy_to_image(image).save(os.path.join(folder, str(i) + '_image.png'))\n","\n","        detected_bbox = detect_function.nms(bboxes)\n","        # print([b[-1] for b in detected_bbox])\n","        # print(len(detected_bbox))\n","        image = data_function.add_bbox_to_image(image, detected_bbox)\n","        # image.save(os.path.join(folder, name.replace('.png', '_bbox.png')))\n","        image.save(os.path.join(folder, str(i) + '_bbox.png'))\n","\n","\n","\n","def compute_detection_metric(outputs, labels, loss_outputs,metric_dict):\n","    loss_outputs[0] = loss_outputs[0].data\n","    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n","\n","def compute_segmentation_metric(outputs, labels, loss_outputs, metric_dict):\n","    loss_outputs[0] = loss_outputs[0].data\n","    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n","\n","def compute_metric(outputs, labels, time, loss_outputs,metric_dict, phase='train'):\n","    # loss_output_list, f1score_list, recall_list, precision_list):\n","    if phase != 'test':\n","        preds = outputs.data.cpu().numpy()\n","        labels = labels.data.cpu().numpy()\n","    else:\n","        preds = np.array(outputs)\n","\n","    preds = preds.reshape(-1)\n","    labels = labels.reshape(-1)\n","\n","    if time is not None:\n","        time = time.reshape(-1)\n","        assert preds.shape == time.shape\n","        time = time[labels>-0.5]\n","    assert preds.shape == labels.shape\n","\n","    preds = preds[labels>-0.5]\n","    label = labels[labels>-0.5]\n","\n","    pred = preds > 0\n","\n","    assert len(pred) == len(label)\n","\n","    tp = (pred + label == 2).sum()\n","    tn = (pred + label == 0).sum()\n","    fp = (pred - label == 1).sum()\n","    fn = (pred - label ==-1).sum()\n","    fp = (pred - label == 1).sum()\n","\n","    metric_dict['tp'] = metric_dict.get('tp', 0.0) + tp\n","    metric_dict['tn'] = metric_dict.get('tn', 0.0) + tn\n","    metric_dict['fp'] = metric_dict.get('fp', 0.0) + fp\n","    metric_dict['fn'] = metric_dict.get('fn', 0.0) + fn\n","    loss = []\n","    for x in loss_outputs:\n","        if x == 0:\n","            loss.append(x)\n","        else:\n","            loss.append(x.data.cpu().numpy())\n","    # loss = [[x.data.cpu().numpy() for x in loss_outputs]]\n","    metric_dict['loss'] = metric_dict.get('loss', []) +  [loss]\n","    if phase != 'train':\n","        metric_dict['preds'] = metric_dict.get('preds', []) + list(preds)\n","        metric_dict['labels'] = metric_dict.get('labels', []) + list(label)\n","        if time is not None:\n","            metric_dict['times'] = metric_dict.get('times', []) + list(time)\n","\n","def compute_metric_multi_classification(outputs, labels, loss_outputs, metric_dict):\n","    preds = outputs.data.cpu().numpy() > 0\n","    labels = labels.data.cpu().numpy()\n","    for pred, label in zip(preds, labels):\n","        pred = np.argmax(pred)\n","        tp = (pred == label ).sum()\n","        fn = (pred != label).sum()\n","        accuracy = 1.0 * tp / (tp + fn)\n","        metric_dict['accuracy'] = metric_dict.get('accuracy', []) + [accuracy]\n","    metric_dict['loss'] = metric_dict.get('loss', []) +  [[x.data.cpu().numpy() for x in loss_outputs]]\n","\n","\n","def print_metric(first_line, metric_dict, phase='train'):\n","    print(first_line)\n","    loss_array = np.array(metric_dict['loss']).mean(0)\n","    tp = metric_dict['tp']\n","    tn = metric_dict['tn']\n","    fp = metric_dict['fp']\n","    fn = metric_dict['fn']\n","    accuracy = 1.0 * (tp + tn) / (tp + tn + fp + fn)\n","    recall = 1.0 * tp / (tp + fn + 10e-20)\n","    precision = 1.0 * tp / (tp + fp + 10e-20)\n","    f1score = 2.0 * recall * precision / (recall + precision + 10e-20)\n","\n","\n","    \n","    loss_array = loss_array.reshape(-1)\n","\n","    print('loss: {:3.4f}\\t pos loss: {:3.4f}\\t negloss: {:3.4f}'.format(loss_array[0], loss_array[1], loss_array[2]))\n","    print('accuracy: {:3.4f}\\t f1score: {:3.4f}\\t recall: {:3.4f}\\t precision: {:3.4f}'.format(accuracy, f1score, recall, precision))\n","    print('\\n')\n","\n","    if phase != 'train':\n","        fpr, tpr, thr = metrics.roc_curve(metric_dict['labels'], metric_dict['preds'])\n","        return metrics.auc(fpr, tpr)\n","    else:\n","        return f1score\n","\n","# def load_all():\n","#     fo = '/content/drive/MyDrive/sepsismodel/code/models'\n","#     pre = ''\n","#     for fi in sorted(os.listdir(fo)):\n","#         if fi[:5] != pre:\n","#             print\n","#             pre = fi[:5]\n","#         print(os.path.join(fo, fi))\n","#         x = torch.load(os.path.join(fo, fi))\n","#         print (x['best_metric'], fi)\n","# load_all()\n","\n"],"metadata":{"id":"m6iopo6W_7-x","executionInfo":{"status":"ok","timestamp":1681547321514,"user_tz":420,"elapsed":403,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","\n","import os\n","import json\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.autograd import *\n","\n","import numpy as np\n","\n","import sys\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n","import parse, py_op\n","args = parse.args\n","\n","args.hard_mining = 0\n","args.gpu = 1\n","args.use_trend = max(args.use_trend, args.use_value)\n","args.use_value = max(args.use_trend, args.use_value)\n","args.rnn_size = args.embed_size\n","args.hidden_size = args.embed_size\n","\n","def time_encoding_data(d = 512, time = 200):\n","    vec = np.array([np.arange(time) * i for i in range(int(d/2))], dtype=np.float32).transpose()\n","    vec = vec / vec.max() / 2\n","    encoding = np.concatenate((np.sin(vec), np.cos(vec)), 1)\n","    encoding = torch.from_numpy(encoding)\n","    return encoding\n","\n","\n","class LSTM(nn.Module):\n","    def __init__(self, opt):\n","        super ( LSTM, self ).__init__ ( )\n","        self.use_cat = args.use_cat\n","        self.avg_time = args.avg_time\n","\n","        self.embedding = nn.Embedding (opt.vocab_size, opt.embed_size )\n","        self.lstm = nn.LSTM ( input_size=opt.embed_size,\n","                              hidden_size=opt.hidden_size,\n","                              num_layers=opt.num_layers,\n","                              batch_first=True,\n","                              bidirectional=True)\n","\n","        self.linear_embed = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size ),\n","            nn.ReLU ( ),\n","            nn.Linear ( opt.embed_size, opt.embed_size ),\n","        )\n","        self.tv_mapping = nn.Sequential (\n","            nn.Linear ( opt.embed_size , int(opt.embed_size / 2)),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( int(opt.embed_size / 2), opt.embed_size ),\n","        )\n","        self.alpha = nn.Linear(args.embed_size, 1)\n","\n","\n","        no = 1\n","        if self.use_cat:\n","            no += 1\n","        self.output_time = nn.Sequential (\n","                nn.Linear(opt.embed_size * no, opt.embed_size),\n","                nn.ReLU ( ),\n","        )\n","\n","        time = 200\n","        self.time_encoding = nn.Embedding.from_pretrained(time_encoding_data(opt.embed_size, time))\n","        self.time_mapping = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( opt.embed_size, opt.embed_size)\n","            )\n","\n","        self.embed_linear = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size),\n","            nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( opt.embed_size, opt.embed_size),\n","            # nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","        )\n","        self.relu = nn.ReLU ( )\n","\n","        self.linears = nn.Sequential (\n","            nn.Linear ( opt.hidden_size * 2, opt.rnn_size ),\n","            # nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( opt.rnn_size, opt.rnn_size ),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( opt.rnn_size, 1),\n","        )\n","        mn = 128\n","        self.master_linear = nn.Sequential (\n","            nn.Linear ( 43, mn),\n","            # nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( mn, mn),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( mn, 1),\n","        )\n","        self.output = nn.Sequential (\n","            nn.Linear ( mn + opt.rnn_size , opt.rnn_size),\n","            nn.ReLU ( ),\n","            nn.Linear ( opt.rnn_size, mn),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( mn, 1),\n","        )\n","        self.pooling = nn.AdaptiveMaxPool1d(1)\n","        self.opt = opt\n","\n","    def visit_pooling(self, x, mask, time, value=None, trend=None):\n","\n","        output = x\n","        size = output.size()\n","        output = output.view(size[0] * size[1], size[2], output.size(3)) # (bs*98, 72, 512)\n","        if args.use_glp:\n","            output = torch.transpose(output, 1,2).contiguous() # (bs*98, 512, 72)\n","            output = self.pooling(output)\n","        else:\n","            weight = self.alpha(output) # (bs*98, 72, 1)\n","            # print weight.size()\n","            weight = weight.view(size[0]*size[1], size[2])\n","            # print weight.size()\n","            weight = F.softmax(weight)\n","            x = weight.data.cpu().numpy()\n","            # print x.shape\n","            weight = weight.view(size[0]*size[1], size[2], 1).expand(output.size())\n","            output = weight * output # (bs*98, 512, 72)\n","            # print output.size()\n","            output = output.sum(1)\n","            # print output.size()\n","            # output = torch.transpose(output, 1,2).contiguous() \n","        output = output.view(size[0], size[1], size[3])\n","\n","        # time encoding\n","        time = - time.long()\n","        time = self.time_encoding(time)\n","        time = self.time_mapping(time)\n","\n","        if self.use_cat:\n","            output = torch.cat((output, time), 2) \n","            output = self.relu(output)\n","            output = self.output_time(output) \n","        else:\n","            output = output + time\n","            output = self.relu(output)\n","\n","\n","\n","        return output\n","\n","\n","    def forward_2(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        '''\n","        task2\n","        '''\n","        size = list(x.size())\n","        x = x.view(-1)\n","        x = self.embedding( x )\n","        x = self.embed_linear(x)\n","        size.append(-1)\n","        x = x.view(size)\n","        if mask is not None:\n","            x = self.visit_pooling(x, mask, time, value, trend)\n","        lstm_out, _ = self.lstm( x )\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 512, 98)\n","        mask = self.pooling(mask)\n","        # print 'mask', mask.size()\n","        pool_out = []\n","        mask_out = []\n","        time_out = []\n","        time = time.data.cpu().numpy()\n","        if phase == 'train':\n","            start, delta = 4, 6\n","        else:\n","            start, delta = 1, 1\n","        for i in range(start, lstm_out.size(2), delta):\n","            pool_out.append(self.pooling(lstm_out[:,:, :i]))\n","            mask_out.append(mask[:, i])\n","            time_out.append(time[:, i])\n","        pool_out.append(self.pooling(lstm_out))\n","        mask_out.append(mask[:, 0])\n","        time_out.append(np.zeros(size[0]) - 4)\n","\n","        lstm_out = torch.cat(pool_out, 2)  # (bs, 512, 98)\n","        mask_out = torch.cat(mask_out, 1)  # (bs, 98)\n","        time_out = np.array(time_out).transpose() # (bs, 98)\n","\n","        # print 'lstm_out', lstm_out.size()\n","        # print 'mask_out', mask_out.size()\n","        # print err\n","\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 98, 512)\n","\n","        out_vital = self.linears(lstm_out)\n","        size = list(out_vital.size())\n","        out_vital = out_vital.view(size[:2])\n","        out_master = self.master_linear(master).expand(size[:2])\n","        out = out_vital + out_master\n","        return out, mask_out, time_out\n","\n","    def forward_1(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        # out = self.master_linear(master)\n","        size = list(x.size())\n","        x = x.view(-1)\n","        x = self.embedding( x )\n","        # print x.size()\n","        x = self.embed_linear(x)\n","        size.append(-1)\n","        x = x.view(size)\n","        if mask is not None:\n","            x = self.visit_pooling(x, mask, time, value, trend)\n","        lstm_out, _ = self.lstm( x )\n","\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()\n","        lstm_out = self.pooling(lstm_out)\n","        lstm_out = lstm_out.view(lstm_out.size(0), -1)\n","\n","        out = self.linears(lstm_out) + self.master_linear(master)\n","        return out\n","\n","    def forward(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        if args.task == 'task2':\n","            return self.forward_2(x, master, mask, time, phase, value, trend)\n","            # return self.forward_1(x, master, mask, time, phase, value, trend)\n","        else:\n","            return self.forward_1(x, master, mask, time, phase, value, trend)"],"metadata":{"id":"QV90XDaYDV-G","executionInfo":{"status":"ok","timestamp":1681547323064,"user_tz":420,"elapsed":3,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#Main run (No6)\n","import os\n","import sys\n","import time\n","import json\n","import traceback\n","import numpy as np\n","from glob import glob\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","# torch\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import DataLoader\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n","#import parse, \n","import py_op\n","\n","os.chdir('/content/drive/MyDrive/sepsismodel/code')\n","import loss\n","#import function\n","\n","# import framework\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/loaddata')\n","import dataloader\n","\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/models')\n","import lstm\n","\n","def train_eval(p_dict, phase='train'):\n","    epoch = p_dict['epoch']\n","    model = p_dict['model']           # 模型\n","    loss = p_dict['loss']             # loss 函数\n","    if phase == 'train':\n","        data_loader = p_dict['train_loader']        # 训练数据\n","        optimizer = p_dict['optimizer']             # 优化器\n","    else:\n","        data_loader = p_dict['val_loader']\n","\n","    classification_metric_dict = dict()\n","    # if args.task == 'case1':\n","\n","    for i,data in enumerate(tqdm(data_loader)):\n","        if args.use_visit:\n","            if args.gpu:\n","                data = [ Variable(x.cuda()) for x in data ]\n","            visits, values, mask, master, labels, times, trends  = data\n","            if i == 0:\n","                print('input size', visits.size())\n","            output = model(visits, master, mask, times, phase, values, trends)\n","        else:\n","            inputs = Variable(data[0].cuda())\n","            labels = Variable(data[1].cuda())\n","            output = model(inputs)\n","\n","        # if 0:\n","        if args.task == 'task2':\n","            output, mask, time = output\n","            labels = labels.unsqueeze(-1).expand(output.size()).contiguous()\n","            labels[mask==0] = -1\n","        else:\n","            time = None\n","\n","        classification_loss_output = loss(output, labels, args.hard_mining)\n","        loss_gradient = classification_loss_output[0]\n","        compute_metric(output, labels, time, classification_loss_output, classification_metric_dict, phase)\n","\n","        # print(outputs.size(), labels.size(),data[3].size(),segment_line_output.size())\n","        # print('detection', detect_character_labels.size(), detect_character_output.size())\n","        # return\n","\n","        if phase == 'train':\n","            optimizer.zero_grad()\n","            loss_gradient.backward()\n","            optimizer.step()\n","\n","        # if i >= 10:\n","        #     break\n","\n","\n","    print('\\nEpoch: {:d} \\t Phase: {:s} \\n'.format(epoch, phase))\n","    metric = print_metric('classification', classification_metric_dict, phase)\n","    if args.phase != 'train':\n","        print('metric = ', metric,'\\n','\\n')\n","        return\n","    if phase == 'val':\n","        if metric > p_dict['best_metric'][0]:\n","            p_dict['best_metric'] = [metric, epoch]\n","            save_model(p_dict)\n","            if 0:\n","            # if args.task == 'task2':\n","                preds = classification_metric_dict['preds'] \n","                labels = classification_metric_dict['labels'] \n","                times = classification_metric_dict['times'] \n","                fl = open('/content/drive/MyDrive/sepsismodel/result/tauc_label.csv', 'w')\n","                fr = open('/content/drive/MyDrive/sepsismodel/result/tauc_result.csv', 'w')\n","                fl.write('adm_id,last_event_time,mortality\\n')\n","                fr.write('adm_id,probability\\n')\n","                for i, (p,l,t) in enumerate(zip(preds, labels, times)):\n","                    if i % 30:\n","                        continue\n","                    fl.write(str(i) + ',')\n","                    fl.write(str(t) + ',')\n","                    fl.write(str(int(l)) + '\\n')\n","\n","                    fr.write(str(i) + ',')\n","                    fr.write(str(p) + '\\n')\n","\n","\n","        print('valid: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n","        print('\\t\\t\\t valid: best_metric: {:3.4f}\\t epoch: {:d}\\n'.format(p_dict['best_metric'][0], p_dict['best_metric'][1]))  \n","    else:\n","        print('train: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n","\n","\n","\n","def main():\n","    p_dict = dict() # All the parameters\n","    p_dict['args'] = args\n","    args.split_nn = args.split_num + args.split_nor * 3\n","    args.vocab_size = args.split_nn * 145 + 1\n","    print('vocab_size', args.vocab_size)\n","\n","    ### load data\n","    print('read data ...')\n","    patient_time_record_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json')\n","    patient_master_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json')\n","    patient_label_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json')\n","\n","    patient_train = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'train.json'))))\n","    patient_valid = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'val.json')))) \n","\n","    if len(patient_train) > len(patient_label_dict):\n","        # patients = patient_time_record_dict.keys()\n","        patients = list(patient_label_dict.keys())\n","        n = int(0.8 * len(patients))\n","        patient_train = patients[:n]\n","        patient_valid = patients[n:]\n","\n","    print('data loading ...')\n","    train_dataset  = dataloader.DataSet(\n","                patient_train, \n","                patient_time_record_dict,\n","                patient_label_dict,\n","                patient_master_dict, \n","                args=args,\n","                phase='train')\n","    train_loader = DataLoader(\n","                dataset=train_dataset, \n","                batch_size=args.batch_size,\n","                shuffle=True, \n","                num_workers=2, \n","                pin_memory=True)\n","    val_dataset  = dataloader.DataSet(\n","                patient_valid, \n","                patient_time_record_dict,\n","                patient_label_dict,\n","                patient_master_dict, \n","                args=args,\n","                phase='val')\n","    val_loader = DataLoader(\n","                dataset=val_dataset, \n","                batch_size=args.batch_size,\n","                shuffle=False, \n","                num_workers=2, \n","                pin_memory=True)\n","\n","    p_dict['train_loader'] = train_loader\n","    p_dict['val_loader'] = val_loader\n","\n","\n","\n","    cudnn.benchmark = False\n","    net = LSTM(args)\n","    if args.gpu:\n","        net = net.cuda()\n","        p_dict['loss'] = loss.Loss().cuda()\n","    else:\n","        p_dict['loss'] = loss.Loss()\n","\n","    parameters = []\n","    for p in net.parameters():\n","        parameters.append(p)\n","    optimizer = torch.optim.Adam(parameters, lr=args.lr)\n","    p_dict['optimizer'] = optimizer\n","    p_dict['model'] = net\n","    start_epoch = 0\n","    # args.epoch = start_epoch\n","    # print ('best_f1score' + str(best_f1score))\n","\n","    p_dict['epoch'] = 0\n","    p_dict['best_metric'] = [0, 0]\n","\n","\n","    ### resume pretrained model\n","    if os.path.exists(args.resume):\n","        print('resume from model ' + args.resume)\n","        function.load_model(p_dict, args.resume)\n","        print('best_metric', p_dict['best_metric'])\n","        # return\n","\n","\n","    if args.phase == 'train':\n","\n","        best_f1score = 0\n","        for epoch in range(p_dict['epoch'] + 1, args.epochs):\n","            p_dict['epoch'] = epoch\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = args.lr\n","            train_eval(p_dict, 'train')\n","            train_eval(p_dict, 'val')\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utQTeEMS7gTf","executionInfo":{"status":"ok","timestamp":1681547337565,"user_tz":420,"elapsed":12967,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"65b1384c-1c77-4205-bb94-c7571de7f71b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size 2031\n","read data ...\n","data loading ...\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 1/3 [00:00<00:00,  2.65it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  5.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 1 \t Phase: train \n","\n","classification\n","loss: 0.6909\t pos loss: 0.3527\t negloss: 0.3382\n","accuracy: 0.4583\t f1score: 0.2353\t recall: 0.2143\t precision: 0.2609\n","\n","\n","train: metric: 0.2353\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 1 \t Phase: val \n","\n","classification\n","loss: 0.6965\t pos loss: 0.3551\t negloss: 0.3414\n","accuracy: 0.4444\t f1score: 0.1667\t recall: 0.2000\t precision: 0.1429\n","\n","\n","valid: metric: 0.4615\t epoch: 1\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 2 \t Phase: train \n","\n","classification\n","loss: 0.7012\t pos loss: 0.3577\t negloss: 0.3434\n","accuracy: 0.4722\t f1score: 0.2963\t recall: 0.2857\t precision: 0.3077\n","\n","\n","train: metric: 0.2963\t epoch: 2\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 2 \t Phase: val \n","\n","classification\n","loss: 0.7050\t pos loss: 0.3580\t negloss: 0.3470\n","accuracy: 0.4444\t f1score: 0.2857\t recall: 0.4000\t precision: 0.2222\n","\n","\n","valid: metric: 0.3077\t epoch: 2\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.84it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 3 \t Phase: train \n","\n","classification\n","loss: 0.6925\t pos loss: 0.3500\t negloss: 0.3424\n","accuracy: 0.5417\t f1score: 0.4000\t recall: 0.3929\t precision: 0.4074\n","\n","\n","train: metric: 0.4000\t epoch: 3\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 3 \t Phase: val \n","\n","classification\n","loss: 0.7037\t pos loss: 0.3536\t negloss: 0.3500\n","accuracy: 0.3333\t f1score: 0.2500\t recall: 0.4000\t precision: 0.1818\n","\n","\n","valid: metric: 0.3077\t epoch: 3\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 4 \t Phase: train \n","\n","classification\n","loss: 0.6820\t pos loss: 0.3436\t negloss: 0.3384\n","accuracy: 0.4861\t f1score: 0.3729\t recall: 0.3929\t precision: 0.3548\n","\n","\n","train: metric: 0.3729\t epoch: 4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 4 \t Phase: val \n","\n","classification\n","loss: 0.7045\t pos loss: 0.3547\t negloss: 0.3498\n","accuracy: 0.4444\t f1score: 0.3750\t recall: 0.6000\t precision: 0.2727\n","\n","\n","valid: metric: 0.3692\t epoch: 4\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 5 \t Phase: train \n","\n","classification\n","loss: 0.6878\t pos loss: 0.3416\t negloss: 0.3462\n","accuracy: 0.5556\t f1score: 0.5000\t recall: 0.5714\t precision: 0.4444\n","\n","\n","train: metric: 0.5000\t epoch: 5\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 5 \t Phase: val \n","\n","classification\n","loss: 0.7043\t pos loss: 0.3511\t negloss: 0.3532\n","accuracy: 0.5000\t f1score: 0.4000\t recall: 0.6000\t precision: 0.3000\n","\n","\n","valid: metric: 0.4000\t epoch: 5\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 6 \t Phase: train \n","\n","classification\n","loss: 0.6932\t pos loss: 0.3454\t negloss: 0.3478\n","accuracy: 0.4167\t f1score: 0.3438\t recall: 0.3929\t precision: 0.3056\n","\n","\n","train: metric: 0.3438\t epoch: 6\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 6 \t Phase: val \n","\n","classification\n","loss: 0.7107\t pos loss: 0.3535\t negloss: 0.3572\n","accuracy: 0.1667\t f1score: 0.1176\t recall: 0.2000\t precision: 0.0833\n","\n","\n","valid: metric: 0.0923\t epoch: 6\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 7 \t Phase: train \n","\n","classification\n","loss: 0.6880\t pos loss: 0.3396\t negloss: 0.3484\n","accuracy: 0.5556\t f1score: 0.5294\t recall: 0.6429\t precision: 0.4500\n","\n","\n","train: metric: 0.5294\t epoch: 7\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 7 \t Phase: val \n","\n","classification\n","loss: 0.7091\t pos loss: 0.3534\t negloss: 0.3557\n","accuracy: 0.2778\t f1score: 0.0000\t recall: 0.0000\t precision: 0.0000\n","\n","\n","valid: metric: 0.2308\t epoch: 7\n","\n","\t\t\t valid: best_metric: 0.4615\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.89it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 8 \t Phase: train \n","\n","classification\n","loss: 0.6848\t pos loss: 0.3430\t negloss: 0.3418\n","accuracy: 0.5000\t f1score: 0.4375\t recall: 0.5000\t precision: 0.3889\n","\n","\n","train: metric: 0.4375\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 8 \t Phase: val \n","\n","classification\n","loss: 0.6850\t pos loss: 0.3322\t negloss: 0.3528\n","accuracy: 0.5000\t f1score: 0.4706\t recall: 0.8000\t precision: 0.3333\n","\n","\n","valid: metric: 0.6769\t epoch: 8\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 9 \t Phase: train \n","\n","classification\n","loss: 0.6846\t pos loss: 0.3381\t negloss: 0.3465\n","accuracy: 0.5556\t f1score: 0.5000\t recall: 0.5714\t precision: 0.4444\n","\n","\n","train: metric: 0.5000\t epoch: 9\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 9 \t Phase: val \n","\n","classification\n","loss: 0.7085\t pos loss: 0.3534\t negloss: 0.3550\n","accuracy: 0.4444\t f1score: 0.3750\t recall: 0.6000\t precision: 0.2727\n","\n","\n","valid: metric: 0.3538\t epoch: 9\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 10 \t Phase: train \n","\n","classification\n","loss: 0.6867\t pos loss: 0.3408\t negloss: 0.3458\n","accuracy: 0.5972\t f1score: 0.5085\t recall: 0.5357\t precision: 0.4839\n","\n","\n","train: metric: 0.5085\t epoch: 10\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 10 \t Phase: val \n","\n","classification\n","loss: 0.6846\t pos loss: 0.3388\t negloss: 0.3458\n","accuracy: 0.5556\t f1score: 0.4286\t recall: 0.6000\t precision: 0.3333\n","\n","\n","valid: metric: 0.6615\t epoch: 10\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 11 \t Phase: train \n","\n","classification\n","loss: 0.6761\t pos loss: 0.3405\t negloss: 0.3357\n","accuracy: 0.6389\t f1score: 0.5667\t recall: 0.6071\t precision: 0.5312\n","\n","\n","train: metric: 0.5667\t epoch: 11\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 11 \t Phase: val \n","\n","classification\n","loss: 0.7015\t pos loss: 0.3488\t negloss: 0.3527\n","accuracy: 0.3333\t f1score: 0.2500\t recall: 0.4000\t precision: 0.1818\n","\n","\n","valid: metric: 0.3846\t epoch: 11\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 12 \t Phase: train \n","\n","classification\n","loss: 0.6875\t pos loss: 0.3454\t negloss: 0.3421\n","accuracy: 0.5972\t f1score: 0.5397\t recall: 0.6071\t precision: 0.4857\n","\n","\n","train: metric: 0.5397\t epoch: 12\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 12 \t Phase: val \n","\n","classification\n","loss: 0.6823\t pos loss: 0.3316\t negloss: 0.3507\n","accuracy: 0.5000\t f1score: 0.5263\t recall: 1.0000\t precision: 0.3571\n","\n","\n","valid: metric: 0.6769\t epoch: 12\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 13 \t Phase: train \n","\n","classification\n","loss: 0.6599\t pos loss: 0.3245\t negloss: 0.3354\n","accuracy: 0.7361\t f1score: 0.6780\t recall: 0.7143\t precision: 0.6452\n","\n","\n","train: metric: 0.6780\t epoch: 13\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 13 \t Phase: val \n","\n","classification\n","loss: 0.6948\t pos loss: 0.3353\t negloss: 0.3595\n","accuracy: 0.3889\t f1score: 0.3529\t recall: 0.6000\t precision: 0.2500\n","\n","\n","valid: metric: 0.4923\t epoch: 13\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n","\n","Epoch: 14 \t Phase: train \n","\n","classification\n","loss: 0.6592\t pos loss: 0.3177\t negloss: 0.3415\n","accuracy: 0.6389\t f1score: 0.5667\t recall: 0.6071\t precision: 0.5312\n","\n","\n","train: metric: 0.5667\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 14 \t Phase: val \n","\n","classification\n","loss: 0.6918\t pos loss: 0.3240\t negloss: 0.3679\n","accuracy: 0.4444\t f1score: 0.4444\t recall: 0.8000\t precision: 0.3077\n","\n","\n","valid: metric: 0.5231\t epoch: 14\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.84it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 15 \t Phase: train \n","\n","classification\n","loss: 0.6445\t pos loss: 0.3089\t negloss: 0.3355\n","accuracy: 0.7361\t f1score: 0.6885\t recall: 0.7500\t precision: 0.6364\n","\n","\n","train: metric: 0.6885\t epoch: 15\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.80it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 15 \t Phase: val \n","\n","classification\n","loss: 0.6850\t pos loss: 0.3323\t negloss: 0.3527\n","accuracy: 0.4444\t f1score: 0.2857\t recall: 0.4000\t precision: 0.2222\n","\n","\n","valid: metric: 0.6154\t epoch: 15\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.73it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  5.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 16 \t Phase: train \n","\n","classification\n","loss: 0.6237\t pos loss: 0.3132\t negloss: 0.3105\n","accuracy: 0.7639\t f1score: 0.6792\t recall: 0.6429\t precision: 0.7200\n","\n","\n","train: metric: 0.6792\t epoch: 16\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.13it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 16 \t Phase: val \n","\n","classification\n","loss: 0.6839\t pos loss: 0.3541\t negloss: 0.3299\n","accuracy: 0.5556\t f1score: 0.3333\t recall: 0.4000\t precision: 0.2857\n","\n","\n","valid: metric: 0.4923\t epoch: 16\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.28it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  4.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 17 \t Phase: train \n","\n","classification\n","loss: 0.6111\t pos loss: 0.3207\t negloss: 0.2904\n","accuracy: 0.7778\t f1score: 0.6800\t recall: 0.6071\t precision: 0.7727\n","\n","\n","train: metric: 0.6800\t epoch: 17\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 17 \t Phase: val \n","\n","classification\n","loss: 0.7022\t pos loss: 0.3450\t negloss: 0.3572\n","accuracy: 0.5000\t f1score: 0.3077\t recall: 0.4000\t precision: 0.2500\n","\n","\n","valid: metric: 0.5077\t epoch: 17\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.44it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 18 \t Phase: train \n","\n","classification\n","loss: 0.5626\t pos loss: 0.2707\t negloss: 0.2918\n","accuracy: 0.7778\t f1score: 0.7143\t recall: 0.7143\t precision: 0.7143\n","\n","\n","train: metric: 0.7143\t epoch: 18\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n","\n","Epoch: 18 \t Phase: val \n","\n","classification\n","loss: 0.6589\t pos loss: 0.3792\t negloss: 0.2796\n","accuracy: 0.7222\t f1score: 0.4444\t recall: 0.4000\t precision: 0.5000\n","\n","\n","valid: metric: 0.5077\t epoch: 18\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.89it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  5.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 19 \t Phase: train \n","\n","classification\n","loss: 0.5993\t pos loss: 0.3202\t negloss: 0.2791\n","accuracy: 0.7778\t f1score: 0.6522\t recall: 0.5357\t precision: 0.8333\n","\n","\n","train: metric: 0.6522\t epoch: 19\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.80it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 98, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  4.56it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 19 \t Phase: val \n","\n","classification\n","loss: 0.7668\t pos loss: 0.3735\t negloss: 0.3934\n","accuracy: 0.5000\t f1score: 0.3077\t recall: 0.4000\t precision: 0.2500\n","\n","\n","valid: metric: 0.4462\t epoch: 19\n","\n","\t\t\t valid: best_metric: 0.6769\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hI9JQxzJ5N6m"},"execution_count":null,"outputs":[]}]}