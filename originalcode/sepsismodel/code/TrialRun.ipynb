{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12CU8xonnPY2XHAl0-oYa3klnhAAyqM7t","authorship_tag":"ABX9TyMcD8eb2BKOWKAetSw4LFk/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#Prep Data (No1)\n","import os\n","import sys\n","import json\n","\n","master_file = \"/content/drive/MyDrive/sepsismodel/file/master.csv\"\n","\n","def gen_master_feature_list():\n","    # master information\n","    m_set = set()\n","    for i_line,line in enumerate(open(master_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            for i,d in enumerate(data[1:]):\n","                m_set.add(str(i) + d)\n","    return sorted(m_set)\n","\n","def gen_patient_master_dict(master_list):\n","    patient_master_dict = dict()\n","    # master information\n","    master_set = [set() for _ in range(6)]\n","    for i_line,line in enumerate(open(master_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            patient = data[0]\n","            feature = ['0' for _ in range(43)]\n","            for i,d in enumerate(data[1:]):\n","                m = str(i) + d\n","                idx = master_list.index(m)\n","                feature[idx] = '1'\n","            patient_master_dict[patient] = ''.join(feature)\n","    content = json.dumps(patient_master_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    master_list = gen_master_feature_list()\n","    gen_patient_master_dict(master_list)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"qCRX0wO1rX0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["master_file = \"/content/drive/MyDrive/sepsismodel/file/master.csv\"\n","\n","for i_line,line in enumerate(open(master_file)):\n","  if i_line in (0,1):\n","    print(line)\n","  else:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCYxZTqn5zcr","executionInfo":{"status":"ok","timestamp":1682501950940,"user_tz":420,"elapsed":3,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"5f7b5902-039b-4877-bdde-1780c93bef1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["adm_id,gender,race,admission_type,admission_source,care_setting,age_grp\n","\n","A100001,Male,African American,Emergency,Emergency Room,Care Setting Undefined,70~80\n","\n"]}]},{"cell_type":"code","source":["#Prep Data (No2)\n","def gen_patient_time_dict():\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    patient_time_dict = dict()\n","    for i_line,line in enumerate(open(vital_file)):\n","        if 'event_time' not in line:\n","            patient, time = line.strip().split(',')[:2]\n","            patient_time_dict[patient] = max(patient_time_dict.get(patient, 0), float(time))\n","    content = json.dumps(patient_time_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    gen_patient_time_dict()\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"CCUqhVB-fy9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Prep Data (No3)\n","def gen_feature_order_dict():\n","    '''\n","    generate the order of value for each feature\n","    '''\n","\n","    feature_value_order_dict = dict()\n","\n","    # vital information\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    vital_dict = { } # key-valuelist-dict\n","    for i_line,line in enumerate(open(vital_file)):\n","        if i_line % 10000 == 0:\n","            print(i_line)\n","        # if i_line > 10000:\n","        #     break\n","        if i_line == 0:\n","            new_line = ''\n","            vis = 0\n","            for c in line:\n","                if c == '\"':\n","                    vis = (vis + 1) % 2\n","                if vis == 1 and c == ',':\n","                    c = ';'\n","                new_line += c\n","            line = new_line\n","            col_list = line.strip().split(',')[1:]\n","            for col in col_list:\n","                vital_dict[col] = []\n","        else:\n","            ctt_list = line.strip().split(',')[1:]\n","            assert len(ctt_list) == len(col_list)\n","            for col,ctt in zip(col_list, ctt_list):\n","                if len(ctt):\n","                    vital_dict[col].append(float(ctt))\n","        # if i_line > 10000:\n","        #    break\n","        # if i_line % 10000 == 0:\n","        #     print i_line\n","\n","    # add group info\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/similar.json\",'r') as f:\n","        groups = json.loads(f.read())\n","        f.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as g:\n","        feature_index_dict = json.loads(g.read())\n","        g.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/index_feature_list.json\",'r') as h:\n","        index_feature_list = json.loads(h.read())\n","        h.close()\n","    for g in groups:\n","        for k in g:\n","            mg = min(g)\n","            if k != mg:\n","                kf = index_feature_list[k]\n","                mf = index_feature_list[mg]\n","                print(kf,mf)\n","                try:\n","                  vital_dict[mf] = vital_dict[mf] + vital_dict[kf]\n","                  vital_dict.pop(kf)\n","                except:\n","                  pass\n","    print('features', len(vital_dict))\n","\n","    # feature_count_dict = { k: len(v) for k,v in vital_dict.items() }\n","    # py_op.mywritejson(os.path.join(args.file_dir, 'feature_count_dict.json'), feature_count_dict)\n","\n","    ms_list = []\n","    for col in col_list:\n","        if col not in vital_dict:\n","            continue\n","        value_list = sorted(vital_dict[col])\n","        value_order_dict = dict()\n","        value_minorder_dict = dict()\n","        value_maxorder_dict = dict()\n","        for i_value, value in enumerate(value_list):\n","            if value not in value_minorder_dict:\n","                value_minorder_dict[value] = i_value\n","            if value == value_list[-1]:\n","                value_maxorder_dict[value] = len(value_list) - 1\n","                break\n","            if value != value_list[i_value+1]:\n","                value_maxorder_dict[value] = i_value\n","        for value in value_maxorder_dict:\n","            value_order_dict[value] = (value_maxorder_dict[value] + value_minorder_dict[value]) / 2.0 / len(value_list)\n","        feature_value_order_dict[col] = value_order_dict\n","    content = json.dumps(feature_value_order_dict,indent=4,ensure_ascii=False)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/feature_value_order_dict.json\",'w') as f:\n","        f.write(content)\n","\n","def main():\n","    gen_feature_order_dict()\n","\n","if __name__ == '__main__':\n","    #os.system('clear')\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LrU0gPogcG0","executionInfo":{"status":"ok","timestamp":1681662929646,"user_tz":420,"elapsed":413,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"79215b0e-7924-4625-e15b-18bcf0c9d8e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","Base Excess Calc Base Excess\n","\"Base Excess Calc; Arterial\" Base Excess\n","\"CO2 Total; Arterial\" CO2\n","\"CO2 Total; Serum\" CO2\n","\"Calcium; Serum\" Calcium Quant\n","\"Glomerular Filtration Rate; Est\" Glomerular Filtration Rate\n","Glucose WBlood Quant Glucose Stick/Meter WBlood POC\n","\"HCO3; Arterial\" HCO3\n","HR Apical HR\n","HR Monitored HR\n","\"Lymphocytes Cnt Bld Auto;Abs\" Lymph Abs Cnt\n","\"Lymphocytes Cnt Bld Auto;%\" Lymph %\n","Lymphocytes NFr Bld Auto Lymph %\n","\"Magnesium; Serum/Plasma\" Magnesium\n","\"PCO2; Arterial\" PCO2\n","\"PO2; Arterial\" PO2\n","PaO2 PO2\n","PT Time PPP PT\n","Pulse Peripheral Pulse\n","Resp Rt Tot Resp Rt\n","SaO2 SO2\n","\"SaO2 %; Arterial\" SO2\n","Temp Axillary Temp\n","Temp Oral Temp\n","Temp Temporal Artery Temp\n","Temp Tympanic Temp\n","\"Albumin; Serum\" Albumin Quant\n","Neutrophil Seg Neutrophil %\n","\"Nucleated RBC Ratio; Blood Auto\" Diff Nucleated RBC\n","features 116\n"]}]},{"cell_type":"code","source":["#Prep Data (No4)\n","def gen_json_data():\n","    vital_file = \"/content/drive/MyDrive/sepsismodel/file/vital.csv\"\n","    patient_time_record_dict = dict()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_index_dict.json\",'r') as h:\n","        feature_index_dict = json.loads(h.read())\n","        h.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/feature_value_order_dict.json\",'r') as f:\n","        feature_value_order_dict = json.loads(f.read())\n","        f.close()\n","    feature_value_order_dict = { str(feature_index_dict[k]):v for k,v in feature_value_order_dict.items()  if 'event' not in k}\n","    with open(\"/content/drive/MyDrive/sepsismodel/file/index_group_dict.json\",'r') as g:\n","        index_group_dict = json.loads(g.read())\n","        g.close()\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_dict.json\",'r') as i:\n","        patient_time_dict = json.loads(i.read())\n","        i.close()\n","    mx_time = - 100\n","    for i_line, line in enumerate(open(vital_file)):\n","        if i_line % 10000 == 0:\n","            print('line', i_line)\n","        if 'event_time' not in line:\n","            data = line.strip().split(',')\n","            patient, time = data[:2]\n","            time = int(float(time))\n","            mx_time = max(mx_time, time)\n","            if patient not in patient_time_record_dict:\n","                patient_time_record_dict[patient] = dict()\n","            if time not in patient_time_record_dict[patient]:\n","                patient_time_record_dict[patient][time] = dict()\n","\n","            data = data[2:]\n","            vs = dict()\n","            for idx, val in enumerate(data):\n","                if len(val) == 0:\n","                    continue\n","                if str(idx) in index_group_dict:\n","                    idx = index_group_dict[str(idx)]\n","                value_order = feature_value_order_dict[str(idx)]\n","                try:\n","                  vs[idx] = value_order[val]\n","                except:\n","                  pass\n","            patient_time_record_dict[patient][time].update(vs)\n","\n","    new_d = dict()\n","    for p, tr in patient_time_record_dict.items():\n","        new_d[p] = dict()\n","        for t, vs in tr.items():\n","            if mx_time > 0:\n","                t = int(t - patient_time_dict[p] - 4)\n","            if t < - 102:\n","                continue\n","            nvs = []\n","            for k in sorted(vs.keys()):\n","                nvs.append([k, vs[k]])\n","            new_d[p][t] = nvs\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json\", 'w') as f:\n","        f.write(json.dumps(new_d))\n","\n","def main():\n","    gen_json_data()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gek8uWWA24hP","executionInfo":{"status":"ok","timestamp":1681662939434,"user_tz":420,"elapsed":398,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"c1bba663-2983-470a-95a8-6e08eb31612a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["line 0\n"]}]},{"cell_type":"code","source":["#Prep Data (No5)\n","def gen_patient_label_dict():\n","    patient_label_dict = dict()\n","    label_file = \"/content/drive/MyDrive/sepsismodel/file/label.csv\"\n","    for i_line,line in enumerate(open(label_file)):\n","        if i_line != 0:\n","            data = line.strip().split(',')\n","            patient = data[0]\n","            label  = data[-1]\n","            patient_label_dict[patient] = int(label)\n","    with open(\"/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json\", 'w') as f:\n","        f.write(json.dumps(patient_label_dict))\n","        f.close()\n","\n","def main():\n","    gen_patient_label_dict()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"umpx0Dsa6UW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set up arguments\n","\n","import argparse\n","parser = argparse.ArgumentParser(description='SepsisModel')\n","parser.add_argument('-dd',\n","        '--data-dir',\n","        type=str,\n","        default='/content/drive/MyDrive/sepsismodel/',\n","        help='data directory'\n","        )\n","parser.add_argument(\n","        '--result-dir',\n","        type=str,\n","        default='../result/',\n","        help='result directory'\n","        )\n","parser.add_argument(\n","        '--file-dir',\n","        type=str,\n","        default='../file/',\n","        help='useful file directory'\n","        )\n","parser.add_argument(\n","        '--vital-file',\n","        type=str,\n","        default='../file/vital.csv',\n","        help='vital information'\n","        )\n","parser.add_argument(\n","        '--master-file',\n","        type=str,\n","        default='../file/master.csv',\n","        help='master information'\n","        )\n","parser.add_argument(\n","        '--label-file',\n","        type=str,\n","        default='../file/label.csv',\n","        help='label'\n","        )\n","parser.add_argument(\n","        '--model',\n","        '-m',\n","        type=str,\n","        default='lstm',\n","        help='model'\n","        )\n","parser.add_argument(\n","        '--embed-size',\n","        metavar='EMBED SIZE',\n","        type=int,\n","        default=256,\n","        help='embed size'\n","        )\n","parser.add_argument(\n","        '--rnn-size',\n","        metavar='rnn SIZE',\n","        type=int,\n","        help='rnn size'\n","        )\n","parser.add_argument(\n","        '--hidden-size',\n","        metavar='hidden SIZE',\n","        type=int,\n","        help='hidden size'\n","        )\n","parser.add_argument(\n","        '--split-num',\n","        metavar='split num',\n","        type=int,\n","        default=5,\n","        help='split num'\n","        )\n","parser.add_argument(\n","        '--split-nor',\n","        metavar='split normal range',\n","        type=int,\n","        default=3,\n","        help='split num'\n","        )\n","parser.add_argument(\n","        '--num-layers',\n","        metavar='num layers',\n","        type=int,\n","        default=2,\n","        help='num layers'\n","        )\n","parser.add_argument(\n","        '--num-code',\n","        metavar='num codes',\n","        type=int,\n","        default=1200,\n","        help='num code'\n","        )\n","parser.add_argument(\n","        '--use-glp',\n","        metavar='use global pooling operation',\n","        type=int,\n","        default=1,\n","        help='use global pooling operation'\n","        )\n","parser.add_argument(\n","        '--use-visit',\n","        metavar='use visit as input',\n","        type=int,\n","        default=1,\n","        help='use visit as input'\n","        )\n","parser.add_argument(\n","        '--use-value',\n","        metavar='use value embedding as input',\n","        type=int,\n","        default=1,\n","        help='use value embedding as input'\n","        )\n","parser.add_argument(\n","        '--use-cat',\n","        metavar='use cat for time and value embedding',\n","        type=int,\n","        default=1,\n","        help='use cat or add'\n","        )\n","parser.add_argument(\n","        '--use-trend',\n","        metavar='use feature variation trend',\n","        type=int,\n","        default=1,\n","        help='use trend'\n","        )\n","parser.add_argument(\n","        '--avg-time',\n","        metavar='avg time for trend, hours',\n","        type=int,\n","        default=4,\n","        help='avg time for trend'\n","        )\n","parser.add_argument(\n","        '--seed',\n","        metavar='seed',\n","        type=int,\n","        default=1,\n","        help='seed'\n","        )\n","parser.add_argument(\n","        '--set',\n","        metavar='split set for training',\n","        type=int,\n","        default=0,\n","        help='split set'\n","        )\n","parser.add_argument(\n","        '--last-time',\n","        metavar='last-time for task2',\n","        type=int,\n","        default=-4,\n","        help='last time'\n","        )\n","parser.add_argument(\n","        '--final',\n","        metavar='final test to submit',\n","        type=int,\n","        default=0,\n","        help='final'\n","        )\n","\n","parser.add_argument('--phase',\n","        default='train',\n","        type=str,\n","        metavar='S',\n","        help='pretrain/train/test phase')\n","parser.add_argument(\n","        '--batch-size',\n","        '-b',\n","        metavar='BATCH SIZE',\n","        type=int,\n","        default=32,\n","        help='batch size'\n","        )\n","parser.add_argument('--save-dir',\n","        default='../../data',\n","        type=str,\n","        metavar='S',\n","        help='save dir')\n","parser.add_argument('--resume',\n","        default='',\n","        type=str,\n","        metavar='S',\n","        help='start from checkpoints')\n","parser.add_argument('--task',\n","        default='task2',\n","        type=str,\n","        metavar='S',\n","        help='start from checkpoints')\n","\n","#####\n","parser.add_argument('-j',\n","        '--workers',\n","        default=8,\n","        type=int,\n","        metavar='N',\n","        help='number of data loading workers (default: 32)')\n","parser.add_argument('--lr',\n","        '--learning-rate',\n","        default=0.0001,\n","        type=float,\n","        metavar='LR',\n","        help='initial learning rate')\n","parser.add_argument('--epochs',\n","        default=20,\n","        type=int,\n","        metavar='N',\n","        help='number of total epochs to run')\n","parser.add_argument('--save-freq',\n","        default='5',\n","        type=int,\n","        metavar='S',\n","        help='save frequency')\n","parser.add_argument('--save-pred-freq',\n","        default='10',\n","        type=int,\n","        metavar='S',\n","        help='save pred clean frequency')\n","parser.add_argument('--val-freq',\n","        default='5',\n","        type=int,\n","        metavar='S',\n","        help='val frequency')\n","args, unknown = parser.parse_known_args()\n","\n","args.hard_mining = 0\n","args.gpu = 1\n","args.use_trend = max(args.use_trend, args.use_value)\n","args.use_value = max(args.use_trend, args.use_value)\n","args.rnn_size = args.embed_size\n","args.hidden_size = args.embed_size"],"metadata":{"id":"0szwMPBm8rS3","executionInfo":{"status":"ok","timestamp":1682897520976,"user_tz":420,"elapsed":3,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#Function.py\n","\n","def save_model(p_dict, name='best.ckpt', folder='/content/drive/MyDrive/sepsismodel/result/models/'):\n","    args = p_dict['args']\n","    name = '{:s}-snm-{:d}-snr-{:d}-value-{:d}-trend-{:d}-cat-{:d}-lt-{:d}-size-{:d}-seed-{:d}-{:s}'.format(args.task, \n","            args.split_num, args.split_nor, args.use_value, args.use_trend, \n","            args.use_cat, args.last_time, args.embed_size, args.seed, name)\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    model = p_dict['model']\n","    state_dict = model.state_dict()\n","    for key in state_dict.keys():\n","        state_dict[key] = state_dict[key].cpu()\n","    all_dict = {\n","            'epoch': p_dict['epoch'],\n","            'args': p_dict['args'],\n","            'best_metric': p_dict['best_metric'],\n","            'state_dict': state_dict \n","            }\n","    torch.save(all_dict, os.path.join(folder, name))\n","\n","def load_model(p_dict, model_file):\n","    all_dict = torch.load(model_file)\n","    p_dict['epoch'] = all_dict['epoch']\n","    # p_dict['args'] = all_dict['args']\n","    p_dict['best_metric'] = all_dict['best_metric']\n","    # for k,v in all_dict['state_dict'].items():\n","    #     p_dict['model_dict'][k].load_state_dict(all_dict['state_dict'][k])\n","    p_dict['model'].load_state_dict(all_dict['state_dict'])\n","\n","\n","def save_segmentation_results(images, segmentations, folder='../data/middle_segmentation'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","\n","    # images = images.data.cpu().numpy()\n","    # segmentations = segmentations.data.cpu().numpy()\n","    images = (images * 128) + 127\n","    segmentations[segmentations>0] = 255\n","    segmentations[segmentations<0] = 0\n","\n","    # print(images.shape, segmentations.shape)\n","    for ii, image, seg in zip(range(len(images)), images, segmentations):\n","        image = data_function.numpy_to_image(image)\n","        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n","        for i in range(seg.shape[1]):\n","            for j in range(seg.shape[2]):\n","                for k in range(3):\n","                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n","        seg = new_seg\n","        seg = data_function.numpy_to_image(seg)\n","        image.save(os.path.join(folder, str(ii) + '_image.png'))\n","        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n","\n","\n","def save_middle_results(data, folder = '../data/middle_images'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    numpy_data = [x.data.numpy() for x in data[1:]]\n","    data =  data[:1] + numpy_data\n","    image_names, images, word_labels, seg_labels, bbox_labels, bbox_images =  data[:6]\n","    images = (images * 128) + 127\n","    seg_labels = seg_labels*127 + 127\n","\n","\n","    for ii, name, image, seg, bbox_image in zip(range(len(image_names)), image_names, images, seg_labels, bbox_images):\n","        name = name.split('/')[-1]\n","        image = data_function.numpy_to_image(image)\n","        new_seg = np.zeros([3, seg.shape[1] * stride, seg.shape[2] * stride])\n","        # print(seg[0].max(),seg[0].min())\n","        for i in range(seg.shape[1]):\n","            for j in range(seg.shape[2]):\n","                for k in range(3):\n","                    new_seg[k, i*stride:(i+1)*stride, j*stride:(j+1)*stride] = seg[0,i,j]\n","        seg = new_seg\n","        seg = data_function.numpy_to_image(seg)\n","        # image.save(os.path.join(folder, name))\n","        # seg.save(os.path.join(folder, name.replace('image.png', 'seg.png')))\n","        image.save(os.path.join(folder, str(ii) + '_image.png'))\n","        seg.save(os.path.join(folder, str(ii) + '_seg.png'))\n","\n","        for ib,bimg in enumerate(bbox_image):\n","            # print(bimg.max(), bimg.min(), bimg.dtype)\n","            bimg = data_function.numpy_to_image(bimg)\n","            bimg.save(os.path.join(folder, str(ii)+'_'+ str(ib) + '_bbox.png'))\n","\n","def save_detection_results(names, images, detect_character_output, folder='../data/test_results/'):\n","    stride = args.stride\n","\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    # images = images.data.cpu().numpy()                                      # [bs, 3, w, h]\n","    images = (images * 128) + 127\n","    # detect_character_output = detect_character_output.data.cpu().numpy()    # [bs, w, h, n_anchors, 5+class]\n","\n","    for i, name, image, bboxes in zip(range(len(names)), names, images, detect_character_output):\n","        name = name.split('/')[-1]\n","\n","        ### 保存原图\n","        # data_function.numpy_to_image(image).save(os.path.join(folder, name))\n","        data_function.numpy_to_image(image).save(os.path.join(folder, str(i) + '_image.png'))\n","\n","        detected_bbox = detect_function.nms(bboxes)\n","        # print([b[-1] for b in detected_bbox])\n","        # print(len(detected_bbox))\n","        image = data_function.add_bbox_to_image(image, detected_bbox)\n","        # image.save(os.path.join(folder, name.replace('.png', '_bbox.png')))\n","        image.save(os.path.join(folder, str(i) + '_bbox.png'))\n","\n","\n","\n","def compute_detection_metric(outputs, labels, loss_outputs,metric_dict):\n","    loss_outputs[0] = loss_outputs[0].data\n","    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n","\n","def compute_segmentation_metric(outputs, labels, loss_outputs, metric_dict):\n","    loss_outputs[0] = loss_outputs[0].data\n","    metric_dict['metric'] = metric_dict.get('metric', []) + [loss_outputs]\n","\n","def compute_metric(outputs, labels, time, loss_outputs,metric_dict, phase='train'):\n","    # loss_output_list, f1score_list, recall_list, precision_list):\n","    if phase != 'test':\n","        preds = outputs.data.cpu().numpy()\n","        labels = labels.data.cpu().numpy()\n","    else:\n","        preds = np.array(outputs)\n","\n","    preds = preds.reshape(-1)\n","    labels = labels.reshape(-1)\n","\n","    if time is not None:\n","        time = time.reshape(-1)\n","        assert preds.shape == time.shape\n","        time = time[labels>-0.5]\n","    assert preds.shape == labels.shape\n","\n","    preds = preds[labels>-0.5]\n","    label = labels[labels>-0.5]\n","\n","    pred = preds > 0\n","\n","    assert len(pred) == len(label)\n","\n","    tp = (pred + label == 2).sum()\n","    tn = (pred + label == 0).sum()\n","    fp = (pred - label == 1).sum()\n","    fn = (pred - label ==-1).sum()\n","    fp = (pred - label == 1).sum()\n","\n","    metric_dict['tp'] = metric_dict.get('tp', 0.0) + tp\n","    metric_dict['tn'] = metric_dict.get('tn', 0.0) + tn\n","    metric_dict['fp'] = metric_dict.get('fp', 0.0) + fp\n","    metric_dict['fn'] = metric_dict.get('fn', 0.0) + fn\n","    loss = []\n","    for x in loss_outputs:\n","        if x == 0:\n","            loss.append(x)\n","        else:\n","            loss.append(x.data.cpu().numpy())\n","    # loss = [[x.data.cpu().numpy() for x in loss_outputs]]\n","    metric_dict['loss'] = metric_dict.get('loss', []) +  [loss]\n","    if phase != 'train':\n","        metric_dict['preds'] = metric_dict.get('preds', []) + list(preds)\n","        metric_dict['labels'] = metric_dict.get('labels', []) + list(label)\n","        if time is not None:\n","            metric_dict['times'] = metric_dict.get('times', []) + list(time)\n","\n","def compute_metric_multi_classification(outputs, labels, loss_outputs, metric_dict):\n","    preds = outputs.data.cpu().numpy() > 0\n","    labels = labels.data.cpu().numpy()\n","    for pred, label in zip(preds, labels):\n","        pred = np.argmax(pred)\n","        tp = (pred == label ).sum()\n","        fn = (pred != label).sum()\n","        accuracy = 1.0 * tp / (tp + fn)\n","        metric_dict['accuracy'] = metric_dict.get('accuracy', []) + [accuracy]\n","    metric_dict['loss'] = metric_dict.get('loss', []) +  [[x.data.cpu().numpy() for x in loss_outputs]]\n","\n","\n","def print_metric(first_line, metric_dict, phase='train'):\n","    print(first_line)\n","    loss_array = np.array(metric_dict['loss']).mean(0)\n","    tp = metric_dict['tp']\n","    tn = metric_dict['tn']\n","    fp = metric_dict['fp']\n","    fn = metric_dict['fn']\n","    accuracy = 1.0 * (tp + tn) / (tp + tn + fp + fn)\n","    recall = 1.0 * tp / (tp + fn + 10e-20)\n","    precision = 1.0 * tp / (tp + fp + 10e-20)\n","    f1score = 2.0 * recall * precision / (recall + precision + 10e-20)\n","\n","\n","    \n","    loss_array = loss_array.reshape(-1)\n","\n","    print('loss: {:3.4f}\\t pos loss: {:3.4f}\\t negloss: {:3.4f}'.format(loss_array[0], loss_array[1], loss_array[2]))\n","    print('accuracy: {:3.4f}\\t f1score: {:3.4f}\\t recall: {:3.4f}\\t precision: {:3.4f}'.format(accuracy, f1score, recall, precision))\n","    print('\\n')\n","\n","    if phase != 'train':\n","        fpr, tpr, thr = metrics.roc_curve(metric_dict['labels'], metric_dict['preds'])\n","        return metrics.auc(fpr, tpr)\n","    else:\n","        return f1score\n","\n","# def load_all():\n","#     fo = '/content/drive/MyDrive/sepsismodel/code/models'\n","#     pre = ''\n","#     for fi in sorted(os.listdir(fo)):\n","#         if fi[:5] != pre:\n","#             print\n","#             pre = fi[:5]\n","#         print(os.path.join(fo, fi))\n","#         x = torch.load(os.path.join(fo, fi))\n","#         print (x['best_metric'], fi)\n","# load_all()\n","\n"],"metadata":{"id":"m6iopo6W_7-x","executionInfo":{"status":"ok","timestamp":1682897521356,"user_tz":420,"elapsed":2,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","\n","import os\n","import json\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.autograd import *\n","\n","import numpy as np\n","\n","import sys\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n","import parse, py_op\n","#args = parse.args\n","\n","args.hard_mining = 0\n","args.gpu = 1\n","args.use_trend = max(args.use_trend, args.use_value)\n","args.use_value = max(args.use_trend, args.use_value)\n","args.rnn_size = args.embed_size\n","args.hidden_size = args.embed_size\n","\n","def time_encoding_data(d = 512, time = 200):\n","    vec = np.array([np.arange(time) * i for i in range(int(d/2))], dtype=np.float32).transpose()\n","    vec = vec / vec.max() / 2\n","    encoding = np.concatenate((np.sin(vec), np.cos(vec)), 1)\n","    encoding = torch.from_numpy(encoding)\n","    return encoding\n","\n","\n","class LSTM(nn.Module):\n","    def __init__(self, opt):\n","        super ( LSTM, self ).__init__ ( )\n","        self.use_cat = args.use_cat\n","        self.avg_time = args.avg_time\n","\n","        self.embedding = nn.Embedding (opt.vocab_size, opt.embed_size )\n","        self.lstm = nn.LSTM ( input_size=opt.embed_size,\n","                              hidden_size=opt.hidden_size,\n","                              num_layers=opt.num_layers,\n","                              batch_first=True,\n","                              bidirectional=True)\n","\n","        self.linear_embed = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size ),\n","            nn.ReLU ( ),\n","            nn.Linear ( opt.embed_size, opt.embed_size ),\n","        )\n","        self.tv_mapping = nn.Sequential (\n","            nn.Linear ( opt.embed_size , int(opt.embed_size / 2)),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( int(opt.embed_size / 2), opt.embed_size ),\n","        )\n","        self.alpha = nn.Linear(args.embed_size, 1)\n","\n","\n","        no = 1\n","        if self.use_cat:\n","            no += 1\n","        self.output_time = nn.Sequential (\n","                nn.Linear(opt.embed_size * no, opt.embed_size),\n","                nn.ReLU ( ),\n","        )\n","\n","        time = 200\n","        self.time_encoding = nn.Embedding.from_pretrained(time_encoding_data(opt.embed_size, time))\n","        self.time_mapping = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( opt.embed_size, opt.embed_size)\n","            )\n","\n","        self.embed_linear = nn.Sequential (\n","            nn.Linear ( opt.embed_size, opt.embed_size),\n","            nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( opt.embed_size, opt.embed_size),\n","            # nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","        )\n","        self.relu = nn.ReLU ( )\n","\n","        self.linears = nn.Sequential (\n","            nn.Linear ( opt.hidden_size * 2, opt.rnn_size ),\n","            # nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( opt.rnn_size, opt.rnn_size ),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( opt.rnn_size, 1),\n","        )\n","        mn = 128\n","        self.master_linear = nn.Sequential (\n","            nn.Linear ( 43, mn),\n","            # nn.ReLU ( ),\n","            # nn.Dropout ( 0.25 ),\n","            # nn.Linear ( mn, mn),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( mn, 1),\n","        )\n","        self.output = nn.Sequential (\n","            nn.Linear ( mn + opt.rnn_size , opt.rnn_size),\n","            nn.ReLU ( ),\n","            nn.Linear ( opt.rnn_size, mn),\n","            nn.ReLU ( ),\n","            nn.Dropout ( 0.25 ),\n","            nn.Linear ( mn, 1),\n","        )\n","        self.pooling = nn.AdaptiveMaxPool1d(1)\n","        self.opt = opt\n","\n","    def visit_pooling(self, x, mask, time, value=None, trend=None):\n","\n","        output = x\n","        size = output.size()\n","        output = output.view(size[0] * size[1], size[2], output.size(3)) # (bs*98, 72, 512)\n","        if args.use_glp:\n","            output = torch.transpose(output, 1,2).contiguous() # (bs*98, 512, 72)\n","            output = self.pooling(output)\n","        else:\n","            weight = self.alpha(output) # (bs*98, 72, 1)\n","            # print weight.size()\n","            weight = weight.view(size[0]*size[1], size[2])\n","            # print weight.size()\n","            weight = F.softmax(weight)\n","            x = weight.data.cpu().numpy()\n","            # print x.shape\n","            weight = weight.view(size[0]*size[1], size[2], 1).expand(output.size())\n","            output = weight * output # (bs*98, 512, 72)\n","            # print output.size()\n","            output = output.sum(1)\n","            # print output.size()\n","            # output = torch.transpose(output, 1,2).contiguous() \n","        output = output.view(size[0], size[1], size[3])\n","\n","        # time encoding\n","        time = - time.long()\n","        time = self.time_encoding(time)\n","        time = self.time_mapping(time)\n","\n","        if self.use_cat:\n","            output = torch.cat((output, time), 2) \n","            output = self.relu(output)\n","            output = self.output_time(output) \n","        else:\n","            output = output + time\n","            output = self.relu(output)\n","\n","\n","\n","        return output\n","\n","\n","    def forward_2(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        '''\n","        task2\n","        '''\n","        size = list(x.size())\n","        x = x.view(-1)\n","        x = self.embedding( x )\n","        x = self.embed_linear(x)\n","        size.append(-1)\n","        x = x.view(size)\n","        if mask is not None:\n","            x = self.visit_pooling(x, mask, time, value, trend)\n","        lstm_out, _ = self.lstm( x )\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 512, 98)\n","        mask = self.pooling(mask)\n","        # print 'mask', mask.size()\n","        pool_out = []\n","        mask_out = []\n","        time_out = []\n","        time = time.data.cpu().numpy()\n","        if phase == 'train':\n","            start, delta = 4, 6\n","        else:\n","            start, delta = 1, 1\n","        for i in range(start, lstm_out.size(2), delta):\n","            pool_out.append(self.pooling(lstm_out[:,:, :i]))\n","            mask_out.append(mask[:, i])\n","            time_out.append(time[:, i])\n","        pool_out.append(self.pooling(lstm_out))\n","        mask_out.append(mask[:, 0])\n","        time_out.append(np.zeros(size[0]) - 4)\n","\n","        lstm_out = torch.cat(pool_out, 2)  # (bs, 512, 98)\n","        mask_out = torch.cat(mask_out, 1)  # (bs, 98)\n","        time_out = np.array(time_out).transpose() # (bs, 98)\n","\n","        # print 'lstm_out', lstm_out.size()\n","        # print 'mask_out', mask_out.size()\n","        # print err\n","\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous() # (bs, 98, 512)\n","\n","        out_vital = self.linears(lstm_out)\n","        size = list(out_vital.size())\n","        out_vital = out_vital.view(size[:2])\n","        out_master = self.master_linear(master).expand(size[:2])\n","        out = out_vital + out_master\n","        return out, mask_out, time_out\n","\n","    def forward_1(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        # out = self.master_linear(master)\n","        size = list(x.size())\n","        x = x.view(-1)\n","        x = self.embedding( x )\n","        # print x.size()\n","        x = self.embed_linear(x)\n","        size.append(-1)\n","        x = x.view(size)\n","        if mask is not None:\n","            x = self.visit_pooling(x, mask, time, value, trend)\n","        lstm_out, _ = self.lstm( x )\n","\n","        lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()\n","        lstm_out = self.pooling(lstm_out)\n","        lstm_out = lstm_out.view(lstm_out.size(0), -1)\n","\n","        out = self.linears(lstm_out) + self.master_linear(master)\n","        return out\n","\n","    def forward(self, x, master, mask=None, time=None, phase='train', value=None, trend=None):\n","        if args.task == 'task2':\n","            return self.forward_2(x, master, mask, time, phase, value, trend)\n","            # return self.forward_1(x, master, mask, time, phase, value, trend)\n","        else:\n","            return self.forward_1(x, master, mask, time, phase, value, trend)"],"metadata":{"id":"QV90XDaYDV-G","executionInfo":{"status":"ok","timestamp":1682897529833,"user_tz":420,"elapsed":8098,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Main run (No6)\n","import os\n","import sys\n","import time\n","import json\n","import traceback\n","import numpy as np\n","from glob import glob\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","# torch\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import DataLoader\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/tools/')\n","#import parse, \n","import py_op\n","\n","os.chdir('/content/drive/MyDrive/sepsismodel/code')\n","import loss\n","#import function\n","\n","# import framework\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/loaddata')\n","import dataloader\n","\n","os.chdir('/content/drive/MyDrive/sepsismodel/code/models')\n","import lstm\n","\n","def train_eval(p_dict, phase='train'):\n","    epoch = p_dict['epoch']\n","    model = p_dict['model']           \n","    loss = p_dict['loss']             \n","    if phase == 'train':\n","        data_loader = p_dict['train_loader']        \n","        optimizer = p_dict['optimizer']             \n","    else:\n","        data_loader = p_dict['val_loader']\n","\n","    classification_metric_dict = dict()\n","    # if args.task == 'case1':\n","\n","    for i,data in enumerate(tqdm(data_loader)):\n","        if args.use_visit:\n","            if args.gpu:\n","                data = [ Variable(x.cuda()) for x in data ]\n","            visits, values, mask, master, labels, times, trends  = data\n","            if i == 0:\n","                print('input size', visits.size())\n","            output = model(visits, master, mask, times, phase, values, trends)\n","        else:\n","            inputs = Variable(data[0].cuda())\n","            labels = Variable(data[1].cuda())\n","            output = model(inputs)\n","\n","        # if 0:\n","        if args.task == 'task2':\n","            output, mask, time = output\n","            labels = labels.unsqueeze(-1).expand(output.size()).contiguous()\n","            labels[mask==0] = -1\n","        else:\n","            time = None\n","\n","        classification_loss_output = loss(output, labels, args.hard_mining)\n","        loss_gradient = classification_loss_output[0]\n","        compute_metric(output, labels, time, classification_loss_output, classification_metric_dict, phase)\n","\n","        # print(outputs.size(), labels.size(),data[3].size(),segment_line_output.size())\n","        # print('detection', detect_character_labels.size(), detect_character_output.size())\n","        # return\n","\n","        if phase == 'train':\n","            optimizer.zero_grad()\n","            loss_gradient.backward()\n","            optimizer.step()\n","\n","        # if i >= 10:\n","        #     break\n","\n","\n","    print('\\nEpoch: {:d} \\t Phase: {:s} \\n'.format(epoch, phase))\n","    metric = print_metric('classification', classification_metric_dict, phase)\n","    if args.phase != 'train':\n","        print('metric = ', metric,'\\n','\\n')\n","        return\n","    if phase == 'val':\n","        if metric > p_dict['best_metric'][0]:\n","            p_dict['best_metric'] = [metric, epoch]\n","            save_model(p_dict)\n","            if 0:\n","            # if args.task == 'task2':\n","                preds = classification_metric_dict['preds'] \n","                labels = classification_metric_dict['labels'] \n","                times = classification_metric_dict['times'] \n","                fl = open('/content/drive/MyDrive/sepsismodel/result/tauc_label.csv', 'w')\n","                fr = open('/content/drive/MyDrive/sepsismodel/result/tauc_result.csv', 'w')\n","                fl.write('adm_id,last_event_time,mortality\\n')\n","                fr.write('adm_id,probability\\n')\n","                for i, (p,l,t) in enumerate(zip(preds, labels, times)):\n","                    if i % 30:\n","                        continue\n","                    fl.write(str(i) + ',')\n","                    fl.write(str(t) + ',')\n","                    fl.write(str(int(l)) + '\\n')\n","\n","                    fr.write(str(i) + ',')\n","                    fr.write(str(p) + '\\n')\n","\n","\n","        print('valid: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n","        print('\\t\\t\\t valid: best_metric: {:3.4f}\\t epoch: {:d}\\n'.format(p_dict['best_metric'][0], p_dict['best_metric'][1]))  \n","    else:\n","        print('train: metric: {:3.4f}\\t epoch: {:d}\\n'.format(metric, epoch))\n","\n","\n","\n","def main():\n","    p_dict = dict() # All the parameters\n","    p_dict['args'] = args\n","    args.split_nn = args.split_num + args.split_nor * 3\n","    args.vocab_size = args.split_nn * 145 + 1\n","    print('vocab_size', args.vocab_size)\n","\n","    ### load data\n","    print('read data ...')\n","    patient_time_record_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_time_record_dict.json')\n","    patient_master_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_master_dict.json')\n","    patient_label_dict = py_op.myreadjson('/content/drive/MyDrive/sepsismodel/runfiles/patient_label_dict.json')\n","\n","    patient_train = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'train.json'))))\n","    patient_valid = list(json.load(open(os.path.join(\"/content/drive/MyDrive/sepsismodel/file/\", args.task, 'val.json')))) \n","\n","    if len(patient_train) > len(patient_label_dict):\n","        # patients = patient_time_record_dict.keys()\n","        patients = list(patient_label_dict.keys())\n","        n = int(0.8 * len(patients))\n","        patient_train = patients[:n]\n","        patient_valid = patients[n:]\n","\n","    print('data loading ...')\n","    train_dataset  = dataloader.DataSet(\n","                patient_train, \n","                patient_time_record_dict,\n","                patient_label_dict,\n","                patient_master_dict, \n","                args=args,\n","                phase='train')\n","    train_loader = DataLoader(\n","                dataset=train_dataset, \n","                batch_size=args.batch_size,\n","                shuffle=True, \n","                num_workers=2, \n","                pin_memory=True)\n","    val_dataset  = dataloader.DataSet(\n","                patient_valid, \n","                patient_time_record_dict,\n","                patient_label_dict,\n","                patient_master_dict, \n","                args=args,\n","                phase='val')\n","    val_loader = DataLoader(\n","                dataset=val_dataset, \n","                batch_size=args.batch_size,\n","                shuffle=False, \n","                num_workers=2, \n","                pin_memory=True)\n","\n","    p_dict['train_loader'] = train_loader\n","    p_dict['val_loader'] = val_loader\n","\n","\n","\n","    cudnn.benchmark = True\n","    net = LSTM(args)\n","    if args.gpu:\n","        net = net.cuda()\n","        p_dict['loss'] = loss.Loss().cuda()\n","    else:\n","        p_dict['loss'] = loss.Loss()\n","\n","    parameters = []\n","    for p in net.parameters():\n","        parameters.append(p)\n","    optimizer = torch.optim.Adam(parameters, lr=args.lr)\n","    p_dict['optimizer'] = optimizer\n","    p_dict['model'] = net\n","    start_epoch = 0\n","    # args.epoch = start_epoch\n","    # print ('best_f1score' + str(best_f1score))\n","\n","    p_dict['epoch'] = 0\n","    p_dict['best_metric'] = [0, 0]\n","\n","\n","    ### resume pretrained model\n","    if os.path.exists(args.resume):\n","        print('resume from model ' + args.resume)\n","        function.load_model(p_dict, args.resume)\n","        print('best_metric', p_dict['best_metric'])\n","        # return\n","\n","\n","    if args.phase == 'train':\n","\n","        best_f1score = 0\n","        for epoch in range(p_dict['epoch'] + 1, args.epochs):\n","            p_dict['epoch'] = epoch\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = args.lr\n","            train_eval(p_dict, 'train')\n","            train_eval(p_dict, 'val')\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utQTeEMS7gTf","executionInfo":{"status":"ok","timestamp":1682897555932,"user_tz":420,"elapsed":26102,"user":{"displayName":"Cristina Ross","userId":"01901391294122361872"}},"outputId":"e46730df-20c6-4b51-d4ac-394b25617b5a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size 2031\n","read data ...\n","data loading ...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:01<00:00,  2.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 1 \t Phase: train \n","\n","classification\n","loss: 0.6974\t pos loss: 0.3414\t negloss: 0.3560\n","accuracy: 0.4162\t f1score: 0.4420\t recall: 0.5333\t precision: 0.3774\n","\n","\n","train: metric: 0.4420\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 1 \t Phase: val \n","\n","classification\n","loss: 0.6956\t pos loss: 0.3437\t negloss: 0.3519\n","accuracy: 0.5480\t f1score: 0.5294\t recall: 0.8182\t precision: 0.3913\n","\n","\n","valid: metric: 0.4627\t epoch: 1\n","\n","\t\t\t valid: best_metric: 0.4627\t epoch: 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  8.51it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  6.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 2 \t Phase: train \n","\n","classification\n","loss: 0.6836\t pos loss: 0.3360\t negloss: 0.3476\n","accuracy: 0.5087\t f1score: 0.5405\t recall: 0.6667\t precision: 0.4545\n","\n","\n","train: metric: 0.5405\t epoch: 2\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 2 \t Phase: val \n","\n","classification\n","loss: 0.6832\t pos loss: 0.3338\t negloss: 0.3494\n","accuracy: 0.4746\t f1score: 0.4076\t recall: 0.5818\t precision: 0.3137\n","\n","\n","valid: metric: 0.5727\t epoch: 2\n","\n","\t\t\t valid: best_metric: 0.5727\t epoch: 2\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  8.15it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 3 \t Phase: train \n","\n","classification\n","loss: 0.6965\t pos loss: 0.3469\t negloss: 0.3496\n","accuracy: 0.3931\t f1score: 0.4000\t recall: 0.4667\t precision: 0.3500\n","\n","\n","train: metric: 0.4000\t epoch: 3\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  6.44it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 3 \t Phase: val \n","\n","classification\n","loss: 0.6847\t pos loss: 0.3381\t negloss: 0.3466\n","accuracy: 0.5819\t f1score: 0.5256\t recall: 0.7455\t precision: 0.4059\n","\n","\n","valid: metric: 0.6247\t epoch: 3\n","\n","\t\t\t valid: best_metric: 0.6247\t epoch: 3\n","\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 1/3 [00:00<00:00,  2.51it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 4 \t Phase: train \n","\n","classification\n","loss: 0.6867\t pos loss: 0.3351\t negloss: 0.3516\n","accuracy: 0.5607\t f1score: 0.5824\t recall: 0.7067\t precision: 0.4953\n","\n","\n","train: metric: 0.5824\t epoch: 4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.90it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 4 \t Phase: val \n","\n","classification\n","loss: 0.6872\t pos loss: 0.3290\t negloss: 0.3582\n","accuracy: 0.4407\t f1score: 0.4870\t recall: 0.8545\t precision: 0.3406\n","\n","\n","valid: metric: 0.6666\t epoch: 4\n","\n","\t\t\t valid: best_metric: 0.6666\t epoch: 4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 5 \t Phase: train \n","\n","classification\n","loss: 0.6857\t pos loss: 0.3286\t negloss: 0.3571\n","accuracy: 0.4740\t f1score: 0.5427\t recall: 0.7200\t precision: 0.4355\n","\n","\n","train: metric: 0.5427\t epoch: 5\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 5 \t Phase: val \n","\n","classification\n","loss: 0.6952\t pos loss: 0.3312\t negloss: 0.3640\n","accuracy: 0.3898\t f1score: 0.4433\t recall: 0.7818\t precision: 0.3094\n","\n","\n","valid: metric: 0.4936\t epoch: 5\n","\n","\t\t\t valid: best_metric: 0.6666\t epoch: 4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 6 \t Phase: train \n","\n","classification\n","loss: 0.6945\t pos loss: 0.3370\t negloss: 0.3575\n","accuracy: 0.5260\t f1score: 0.5729\t recall: 0.7333\t precision: 0.4701\n","\n","\n","train: metric: 0.5729\t epoch: 6\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 6 \t Phase: val \n","\n","classification\n","loss: 0.6951\t pos loss: 0.3289\t negloss: 0.3662\n","accuracy: 0.2825\t f1score: 0.3351\t recall: 0.5818\t precision: 0.2353\n","\n","\n","valid: metric: 0.4902\t epoch: 6\n","\n","\t\t\t valid: best_metric: 0.6666\t epoch: 4\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 7 \t Phase: train \n","\n","classification\n","loss: 0.6782\t pos loss: 0.3235\t negloss: 0.3547\n","accuracy: 0.5954\t f1score: 0.6535\t recall: 0.8800\t precision: 0.5197\n","\n","\n","train: metric: 0.6535\t epoch: 7\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 7 \t Phase: val \n","\n","classification\n","loss: 0.6843\t pos loss: 0.3237\t negloss: 0.3605\n","accuracy: 0.4689\t f1score: 0.5253\t recall: 0.9455\t precision: 0.3636\n","\n","\n","valid: metric: 0.6696\t epoch: 7\n","\n","\t\t\t valid: best_metric: 0.6696\t epoch: 7\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 8 \t Phase: train \n","\n","classification\n","loss: 0.6860\t pos loss: 0.3316\t negloss: 0.3545\n","accuracy: 0.5549\t f1score: 0.5926\t recall: 0.7467\t precision: 0.4912\n","\n","\n","train: metric: 0.5926\t epoch: 8\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 8 \t Phase: val \n","\n","classification\n","loss: 0.6800\t pos loss: 0.3155\t negloss: 0.3645\n","accuracy: 0.3446\t f1score: 0.4257\t recall: 0.7818\t precision: 0.2925\n","\n","\n","valid: metric: 0.5458\t epoch: 8\n","\n","\t\t\t valid: best_metric: 0.6696\t epoch: 7\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 9 \t Phase: train \n","\n","classification\n","loss: 0.6696\t pos loss: 0.3208\t negloss: 0.3488\n","accuracy: 0.6127\t f1score: 0.6378\t recall: 0.7867\t precision: 0.5364\n","\n","\n","train: metric: 0.6378\t epoch: 9\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 9 \t Phase: val \n","\n","classification\n","loss: 0.6717\t pos loss: 0.3168\t negloss: 0.3550\n","accuracy: 0.4633\t f1score: 0.4865\t recall: 0.8182\t precision: 0.3462\n","\n","\n","valid: metric: 0.6824\t epoch: 9\n","\n","\t\t\t valid: best_metric: 0.6824\t epoch: 9\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  7.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 10 \t Phase: train \n","\n","classification\n","loss: 0.6474\t pos loss: 0.3023\t negloss: 0.3451\n","accuracy: 0.6647\t f1score: 0.6778\t recall: 0.8133\t precision: 0.5810\n","\n","\n","train: metric: 0.6778\t epoch: 10\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  5.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 10 \t Phase: val \n","\n","classification\n","loss: 0.6896\t pos loss: 0.3191\t negloss: 0.3705\n","accuracy: 0.3503\t f1score: 0.3784\t recall: 0.6364\t precision: 0.2692\n","\n","\n","valid: metric: 0.4914\t epoch: 10\n","\n","\t\t\t valid: best_metric: 0.6824\t epoch: 9\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  9.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 11 \t Phase: train \n","\n","classification\n","loss: 0.6557\t pos loss: 0.3148\t negloss: 0.3408\n","accuracy: 0.6763\t f1score: 0.6854\t recall: 0.8133\t precision: 0.5922\n","\n","\n","train: metric: 0.6854\t epoch: 11\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 11 \t Phase: val \n","\n","classification\n","loss: 0.6545\t pos loss: 0.2882\t negloss: 0.3663\n","accuracy: 0.4802\t f1score: 0.5258\t recall: 0.9273\t precision: 0.3669\n","\n","\n","valid: metric: 0.7204\t epoch: 11\n","\n","\t\t\t valid: best_metric: 0.7204\t epoch: 11\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 12.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 12 \t Phase: train \n","\n","classification\n","loss: 0.6679\t pos loss: 0.3329\t negloss: 0.3350\n","accuracy: 0.6705\t f1score: 0.6667\t recall: 0.7600\t precision: 0.5938\n","\n","\n","train: metric: 0.6667\t epoch: 12\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  9.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 12 \t Phase: val \n","\n","classification\n","loss: 0.6460\t pos loss: 0.3001\t negloss: 0.3459\n","accuracy: 0.5932\t f1score: 0.4857\t recall: 0.6182\t precision: 0.4000\n","\n","\n","valid: metric: 0.6808\t epoch: 12\n","\n","\t\t\t valid: best_metric: 0.7204\t epoch: 11\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 11.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 13 \t Phase: train \n","\n","classification\n","loss: 0.5894\t pos loss: 0.2881\t negloss: 0.3013\n","accuracy: 0.7746\t f1score: 0.7068\t recall: 0.6267\t precision: 0.8103\n","\n","\n","train: metric: 0.7068\t epoch: 13\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  9.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 13 \t Phase: val \n","\n","classification\n","loss: 0.6192\t pos loss: 0.3101\t negloss: 0.3090\n","accuracy: 0.7684\t f1score: 0.5941\t recall: 0.5455\t precision: 0.6522\n","\n","\n","valid: metric: 0.7258\t epoch: 13\n","\n","\t\t\t valid: best_metric: 0.7258\t epoch: 13\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 11.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n","\n","Epoch: 14 \t Phase: train \n","\n","classification\n","loss: 0.5954\t pos loss: 0.3054\t negloss: 0.2900\n","accuracy: 0.7688\t f1score: 0.6774\t recall: 0.5600\t precision: 0.8571\n","\n","\n","train: metric: 0.6774\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00, 10.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 14 \t Phase: val \n","\n","classification\n","loss: 0.6153\t pos loss: 0.2812\t negloss: 0.3341\n","accuracy: 0.6328\t f1score: 0.4800\t recall: 0.5455\t precision: 0.4286\n","\n","\n","valid: metric: 0.7261\t epoch: 14\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 1/3 [00:00<00:00,  6.04it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 3/3 [00:00<00:00, 12.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 15 \t Phase: train \n","\n","classification\n","loss: 0.5154\t pos loss: 0.2569\t negloss: 0.2585\n","accuracy: 0.8035\t f1score: 0.7344\t recall: 0.6267\t precision: 0.8868\n","\n","\n","train: metric: 0.7344\t epoch: 15\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 15 \t Phase: val \n","\n","classification\n","loss: 0.6130\t pos loss: 0.2796\t negloss: 0.3334\n","accuracy: 0.6102\t f1score: 0.4000\t recall: 0.4182\t precision: 0.3833\n","\n","\n","valid: metric: 0.7221\t epoch: 15\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 11.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 16 \t Phase: train \n","\n","classification\n","loss: 0.4780\t pos loss: 0.2466\t negloss: 0.2314\n","accuracy: 0.8092\t f1score: 0.7402\t recall: 0.6267\t precision: 0.9038\n","\n","\n","train: metric: 0.7402\t epoch: 16\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  9.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 16 \t Phase: val \n","\n","classification\n","loss: 0.6770\t pos loss: 0.2739\t negloss: 0.4031\n","accuracy: 0.5876\t f1score: 0.4252\t recall: 0.4909\t precision: 0.3750\n","\n","\n","valid: metric: 0.7060\t epoch: 16\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 11.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 17 \t Phase: train \n","\n","classification\n","loss: 0.5064\t pos loss: 0.2306\t negloss: 0.2758\n","accuracy: 0.7919\t f1score: 0.7273\t recall: 0.6400\t precision: 0.8421\n","\n","\n","train: metric: 0.7273\t epoch: 17\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n","\n","Epoch: 17 \t Phase: val \n","\n","classification\n","loss: 0.6208\t pos loss: 0.4126\t negloss: 0.2082\n","accuracy: 0.6949\t f1score: 0.4255\t recall: 0.3636\t precision: 0.5128\n","\n","\n","valid: metric: 0.6435\t epoch: 17\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 12.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 18 \t Phase: train \n","\n","classification\n","loss: 0.6013\t pos loss: 0.4470\t negloss: 0.1543\n","accuracy: 0.7803\t f1score: 0.6607\t recall: 0.4933\t precision: 1.0000\n","\n","\n","train: metric: 0.6607\t epoch: 18\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 18 \t Phase: val \n","\n","classification\n","loss: 0.6996\t pos loss: 0.5635\t negloss: 0.1362\n","accuracy: 0.7910\t f1score: 0.4932\t recall: 0.3273\t precision: 1.0000\n","\n","\n","valid: metric: 0.7231\t epoch: 18\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([32, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00, 11.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 19 \t Phase: train \n","\n","classification\n","loss: 0.6101\t pos loss: 0.4597\t negloss: 0.1504\n","accuracy: 0.7283\t f1score: 0.5437\t recall: 0.3733\t precision: 1.0000\n","\n","\n","train: metric: 0.5437\t epoch: 19\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["input size torch.Size([18, 45, 72])\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 1/1 [00:00<00:00,  9.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 19 \t Phase: val \n","\n","classification\n","loss: 0.6007\t pos loss: 0.4372\t negloss: 0.1635\n","accuracy: 0.8023\t f1score: 0.5333\t recall: 0.3636\t precision: 1.0000\n","\n","\n","valid: metric: 0.6350\t epoch: 19\n","\n","\t\t\t valid: best_metric: 0.7261\t epoch: 14\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hWo5kdc2gUfE"},"execution_count":null,"outputs":[]}]}